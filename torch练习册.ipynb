{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import torch"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"x = torch.empty(5,3)"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[9.2755e-39, 1.0561e-38, 5.0511e-39],\n        [5.0510e-39, 4.2246e-39, 1.0286e-38],\n        [1.0653e-38, 1.0194e-38, 8.4490e-39],\n        [1.0469e-38, 9.3674e-39, 9.9184e-39],\n        [8.7245e-39, 9.2755e-39, 8.9082e-39]])\n"}],"source":"x = torch.empty(5,3, dtype=torch.float32)   # 新建一个空的，数据都是随机值，每次都会变\nprint(x)"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":"tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]], dtype=torch.float16)"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":"x.half()  # 将数据变为半精度"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[0.3620, 0.5660, 0.7610],\n        [0.7748, 0.9409, 0.4492],\n        [0.0780, 0.8547, 0.2295],\n        [0.6775, 0.1408, 0.3257],\n        [0.4907, 0.2096, 0.5018]])\n"}],"source":"y = torch.rand(5,3, dtype=torch.float32)\nprint(y)"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":"tensor([[0.3621, 0.5659, 0.7607],\n        [0.7749, 0.9409, 0.4492],\n        [0.0780, 0.8545, 0.2295],\n        [0.6777, 0.1407, 0.3257],\n        [0.4907, 0.2096, 0.5020]], dtype=torch.float16)"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"y.half()"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]], dtype=torch.float16)\n"}],"source":"zeroTensor = torch.zeros(5,3, dtype=torch.float16)\nprint(zeroTensor)"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([1.2002, 3.0000, 4.0000, 0.0000], dtype=torch.float16)\n"}],"source":"fromLstTensor = torch.tensor([1.2, 3.0, 4, 0], dtype=torch.float16)\nprint(fromLstTensor)"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])torch.float32\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float16)torch.float16\ntensor([[-0.3025, -1.2338,  0.8515],\n        [-0.0760,  0.3108, -0.3506],\n        [-1.2476,  1.3294,  0.1484],\n        [ 1.9713, -1.5333,  0.0382],\n        [ 1.7524,  0.7143,  0.9952]], dtype=torch.float64)torch.float64\n"}],"source":"fromTensorTensor = y.new_ones(3,3)\nprint(fromTensorTensor, fromTensorTensor.dtype)\n\nfromTensorTensor2 = y.new_ones(3,3,dtype=torch.float16)\nprint(fromTensorTensor2, fromTensorTensor2.dtype)\n\nfromTensorTensor3 = torch.randn_like(y, dtype=torch.float64)\nprint(fromTensorTensor3, fromTensorTensor3.dtype)"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"torch.Size([5, 3])torch.Size([5, 3])\ntorch.Size([3, 3])torch.Size([3, 3])\n"}],"source":"print(zeroTensor.shape,  zeroTensor.size())\nprint(fromTensorTensor.shape, fromTensorTensor.size())  # torch.Size 是一个封装过的Tuple"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]], device='cuda:0', dtype=torch.float16)\n\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]], device='cuda:0', dtype=torch.float16)\n\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]], device='cuda:0', dtype=torch.float16)\n"}],"source":"print(fromTensorTensor)\nfromTensorTensor = fromTensorTensor.type(fromTensorTensor2.dtype).cuda()\naddTensor = fromTensorTensor + fromTensorTensor2.cuda()        # dtype类型不一致不能操作， 且float16只支持GPU版本\naddTensor2 = torch.add(fromTensorTensor, fromTensorTensor2.cuda())\nfromTensorTensor.add_(fromTensorTensor2.cuda())                # pytorch的inplace操作后面都加这_\nprint(addTensor,'\\n\\n', addTensor2,'\\n\\n', fromTensorTensor)"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([1.0653e-38, 1.0194e-38, 8.4490e-39])\ntensor([[9.2755e-39, 1.0561e-38, 5.0511e-39],\n        [5.0510e-39, 4.2246e-39, 1.0286e-38],\n        [1.0653e-38, 1.0194e-38, 8.4490e-39],\n        [1.0469e-38, 9.3674e-39, 9.9184e-39],\n        [8.7245e-39, 9.2755e-39, 8.9082e-39]])\ntensor([[9.2755e-39, 1.0561e-38, 5.0511e-39],\n        [5.0510e-39, 4.2246e-39, 1.0286e-38],\n        [1.0000e+00, 1.0000e+00, 1.0000e+00],\n        [1.0469e-38, 9.3674e-39, 9.9184e-39],\n        [8.7245e-39, 9.2755e-39, 8.9082e-39]])\ntensor([0., 0., 0.])\n"}],"source":"m = x[2,:]  # pytorch中索引结果与原数据共享内存，修改其中一个另外一个也会同样变化\nprint(m)\nprint(x)\nm += 1\nprint(x)\nx -= 1\nprint(m)"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([])\n"}],"source":"print(torch.masked_select(x, x>0.5))"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([-1., -1., -1., -1., -1., -1.,  0.,  0.,  0., -1., -1., -1., -1., -1.,\n        -1.])\ntensor([[-1., -1., -1., -1., -1.],\n        [-1.,  0.,  0.,  0., -1.],\n        [-1., -1., -1., -1., -1.]])\ntorch.Size([15])torch.Size([3, 5])torch.Size([5, 3])\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [1., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\ntensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])\ntensor([[0., 0., 0., 0., 0.],\n        [0., 1., 1., 1., 0.],\n        [0., 0., 0., 0., 0.]])\n"}],"source":"# view返回的新tensor与原tensor共享内存，修改一个另外一个也变化。 view只是改变张量的观察角度而已\n# reshape有可能会返回一个拷贝，但不一定啊：  需要不共享内存的数据：建议先clone后view\nviewX = x.view(15)\nprint(viewX)\nviewX2 = x.view(-1,5)\nprint(viewX2)\nprint(viewX.size(), viewX2.size(), x.size())\n\nx += 1\nprint(x,'\\n', viewX, '\\n', viewX2)"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([0.0467])\n0.04670458659529686\n"}],"source":"itemTensor = torch.randn(1)\nprint(itemTensor)\nprint(itemTensor.item())  # 可以将标量Tensor转为一个python number"},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[0., 0., 0., 0., 0.],\n        [0., 1., 1., 1., 0.],\n        [0., 0., 0., 0., 0.]])\ntensor(1.)\ntensor([0., 1., 0.])\ntensor([[0., 0., 0.],\n        [0., 1., 0.],\n        [0., 1., 0.],\n        [0., 1., 0.],\n        [0., 0., 0.]])\n"}],"source":"# pytorch支持线性代数操作\nprint(viewX2)\nprint(torch.trace(viewX2))\nprint(torch.diag(viewX2))\nprint(torch.t(viewX2))"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[1, 2]])tensor([[1],\n        [2],\n        [3]])\ntensor([[2, 3],\n        [3, 4],\n        [4, 5]])\ntorch.Size([3, 2])\n"}],"source":"# pytorch广播机制：适当复制元素使得两个Tensor形状形同后再按元素运算\nt1 = torch.arange(1,3).view(1,2)\nt2 = torch.arange(1,4).view(3,1)\nprint(t1, t2)\nbroadAdd = t1 + t2\nprint(broadAdd, '\\n', broadAdd.shape)"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"False\nTrue\nTrue\nTrue\n"}],"source":"# 运算的内存开销 : += add_ [:] 可以避免重开内存空间\nbeforeT1Id = id(t1)\nt1 = t1 + t2     # 开辟新内存了\nprint(id(t1) == beforeT1Id)\n\nbeforeT1Id = id(t1)\nt1[:] = t1 + t2  # 使用[:]将加法结果写入了t1对应的内存中，避免了内存开销\nprint(id(t1) == beforeT1Id)\n\nbeforeT1Id = id(t1)\nt1 += t2         # 使用+=将加法结果写入了t1对应的内存中，避免了内存开销\nprint(id(t1) == beforeT1Id)\n\nbeforeT1Id = id(t1)\nt1.add_(t2)      # 使用add_将加法结果写入了t1对应的内存中，避免了内存开销\nprint(id(t1) == beforeT1Id)\n"},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[[ 5  6]\n [ 9 10]\n [13 14]]\ntensor([[ 5,  6],\n        [ 9, 10],\n        [13, 14]])\n[[ 6  7]\n [10 11]\n [14 15]]\ntensor([[ 6,  7],\n        [10, 11],\n        [14, 15]])\n[[ 6  7]\n [10 11]\n [14 15]]\ntensor([[ 6,  7],\n        [10, 11],\n        [14, 15]])\n[[ 2  3]\n [ 6  7]\n [10 11]]\ntensor([[ 2,  3],\n        [ 6,  7],\n        [10, 11]])\n"}],"source":"# Tensor与Numpy的相互转换\nnpT1 = t1.numpy()              # 将Tensor转为array, 共享内存\nprint(npT1, '\\n', t1)\nnpT1 += 1\nprint(npT1, '\\n', t1)\n\ntT2 = torch.from_numpy(npT1)   # 将array转为tensor，共享内存\nprint(npT1, '\\n', tT2)\ntT2 -= 4\nprint(npT1, '\\n', tT2)\n\n# 使用torch.tensor()来转化array的话，内存不共享，总是进行数据拷贝\n"},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"True\ntensor([[1, 1],\n        [1, 1],\n        [1, 1]], device='cuda:0')\ntensor([[ 3.,  4.],\n        [ 7.,  8.],\n        [11., 12.]], device='cuda:0', dtype=torch.float16)\ntensor([[ 3.,  4.],\n        [ 7.,  8.],\n        [11., 12.]], dtype=torch.float64)\n"}],"source":"print(torch.cuda.is_available())\ndevice = torch.device('cuda')\ncudaT1 = torch.ones_like(t1, device=device)   #在GPU上新建一个变量\nprint(cudaT1)\n\nt1 = t1.to(device, dtype=torch.float16)       # .to函数可以同时修改对应的dtype\ncudaT2 = cudaT1.type(torch.float16) + t1\nprint(cudaT2, '\\n', cudaT2.to('cpu', torch.double))\n"},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"False\nTrue\n"}],"source":"print(cudaT2.requires_grad)  # cudaT2的requires_grad属性设置为True，将会追踪在当前Tensor上的所有操作\n# 设置好前向计算后，只需要使用.backward()即可完成所有的梯度计算，Tensor的梯度会累积到.grad属性中\ncudaT2.requires_grad_(True)   # 修改属性\nprint(cudaT2.requires_grad)"},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.]], requires_grad=True)\nNone\nTrue\ntensor([[3., 3., 3., 3., 3., 3.],\n        [3., 3., 3., 3., 3., 3.],\n        [3., 3., 3., 3., 3., 3.],\n        [3., 3., 3., 3., 3., 3.],\n        [3., 3., 3., 3., 3., 3.],\n        [3., 3., 3., 3., 3., 3.]], grad_fn=<AddBackward0>)\n<AddBackward0 object at 0x000002C7135E5278>\nTrue\n"}],"source":"bpTensor = torch.ones(6,6, requires_grad=True)\nprint(bpTensor)\nprint(bpTensor.grad_fn, '\\n', bpTensor.is_leaf)         # 直接创建的tensor称为叶子节点，对应的grad_fn为None\n\nfuncBpTensor = bpTensor + 2\nprint(funcBpTensor)\nprint(funcBpTensor.grad_fn, '\\n', funcBpTensor.requires_grad)"},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[3., 3., 3., 3., 3., 3.],\n        [3., 3., 3., 3., 3., 3.],\n        [3., 3., 3., 3., 3., 3.],\n        [3., 3., 3., 3., 3., 3.],\n        [3., 3., 3., 3., 3., 3.],\n        [3., 3., 3., 3., 3., 3.]], grad_fn=<MulBackward0>)\ntensor(3.)\ntensor([[6., 6., 6., 6., 6., 6.],\n        [6., 6., 6., 6., 6., 6.],\n        [6., 6., 6., 6., 6., 6.],\n        [6., 6., 6., 6., 6., 6.],\n        [6., 6., 6., 6., 6., 6.],\n        [6., 6., 6., 6., 6., 6.]])\ntensor([[12., 12., 12., 12., 12., 12.],\n        [12., 12., 12., 12., 12., 12.],\n        [12., 12., 12., 12., 12., 12.],\n        [12., 12., 12., 12., 12., 12.],\n        [12., 12., 12., 12., 12., 12.],\n        [12., 12., 12., 12., 12., 12.]])\n"}],"source":"bpTensor2 = bpTensor * bpTensor * 3\nbpTensor3 = bpTensor2.mean().detach()\nprint(bpTensor2, '\\n', bpTensor3)\nbpTensor2.backward(torch.ones_like(bpTensor), retain_graph=True)               # 因为bpTensor3是一个标量，所以在调用backward时不需要指定求导变量\n# bpTensor3.backward(torch.tensor(1.))                    # RuntimeError：因为detach了\nprint(bpTensor.grad)\nbpTensor2.sum().backward()\nprint(bpTensor.grad)"},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"True\ntensor(1., grad_fn=<PowBackward0>)True\ntensor(1.)False\ntensor(2., grad_fn=<AddBackward0>)True\ntensor(2.)\ntensor(2.)\n"}],"source":"# 终端梯度追踪栗子\nx = torch.tensor(1.0, requires_grad=True)\ny1 = x ** 2\nwith torch.no_grad():\n    y2 = x ** 3\ny3 = y2 + y1\nprint(x.requires_grad)\nprint(y1, y1.requires_grad)\nprint(y2, y2.requires_grad)\nprint(y3, y3.requires_grad)\n\ny1.backward(retain_graph=True)\nprint(x.grad)\n\nx.grad.data.zero_()    # 因为grad会累积，所以在每次反向传播之前都要梯度清零\ny3.backward()\nprint(x.grad)\n\n"},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[0.0198, 0.8136],\n        [0.8066, 0.9191]])\ntensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17.])\ntensor([ 1.0000,  1.9474,  2.8947,  3.8421,  4.7895,  5.7368,  6.6842,  7.6316,\n         8.5789,  9.5263, 10.4737, 11.4211, 12.3684, 13.3158, 14.2632, 15.2105,\n        16.1579, 17.1053, 18.0526, 19.0000])\n"}],"source":"a = torch.rand(2,2, dtype=torch.float32)\nb = torch.arange(1,19,2, dtype=torch.float32)\nc = torch.linspace(1,19,20, dtype=torch.float32)\nprint(a, '\\n', b, '\\n', c)"},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[ 1.0000,  1.9474,  2.8947,  3.8421,  4.7895,  5.7368,  6.6842,  7.6316,\n          8.5789,  9.5263],\n        [10.4737, 11.4211, 12.3684, 13.3158, 14.2632, 15.2105, 16.1579, 17.1053,\n         18.0526, 19.0000]])\ntorch.Size([20])torch.Size([2, 10])\n"}],"source":"d = c.view(2,10)\nprint(d)\nprint(c.shape, d.shape)"},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Sequential(\n  (0): Linear(in_features=10, out_features=1, bias=True)\n)Linear(in_features=10, out_features=1, bias=True)\nSequential(\n  (linear): Linear(in_features=10, out_features=1, bias=True)\n)Linear(in_features=10, out_features=1, bias=True)\nSequential(\n  (linear): Linear(in_features=10, out_features=1, bias=True)\n)Linear(in_features=10, out_features=1, bias=True)\n"}],"source":"##############  新建torch模型，torch.nn：核心数据结构是Module\nfrom torch import nn\n#1：\nnet1 = nn.Sequential(nn.Linear(10, 1))\nprint(net1, net1[0])\n\n#2\nnet2 = nn.Sequential()\nnet2.add_module('linear', nn.Linear(10, 1))\nprint(net2, net2[0])\n\n#3\nfrom collections import OrderedDict\nnet3 = nn.Sequential(OrderedDict([('linear', nn.Linear(10,1))]))\nprint(net3, net3[0])"},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Parameter containing:\ntensor([[ 0.0047, -0.0085, -0.0022,  0.0012,  0.0083, -0.0067, -0.0065, -0.0047,\n          0.0020, -0.0090]], requires_grad=True)\nParameter containing:\ntensor([0.], requires_grad=True)\n"}],"source":"nn.init.normal_(net3[0].weight, mean=0, std=0.01)  # 为权重赋值\nnet3[0].bias.data.fill_(0)                         # 为bias赋值\nfor param in net3.parameters(): print(param)"},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Linear(in_features=10, out_features=1, bias=True)\n"}],"source":"for layer in net3: print(layer)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# torch.optim 提供了很多的模型优化算法：SGD/Adam/RMSProp\nimport "}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}