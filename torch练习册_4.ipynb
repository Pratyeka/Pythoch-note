{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"import torch.nn as nn"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"# 使用nn.Module来构造更加灵活的模型，Module是所有模型的基类\n# 新建模型需要重载Module类的__init__函数和forward函数, 无需定义反向传播函数\n# 系统通过自动求梯度生成反向传播所需的backward函数\n\nclass MLP(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)  # super(MLP, self).__init__(**kwargs)\n        self.hidden = nn.Linear(784, 256)\n        self.act = nn.ReLU()\n        self.output = nn.Linear(256, 10)\n    def forward(self, x):\n        act_x = self.act(self.hidden(x))\n        return self.output(act_x)"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([ 0.1967,  0.0433,  0.1620,  0.0980, -0.0162,  0.4275,  0.0996,  0.0702,\n         0.1785,  0.3364], grad_fn=<AddBackward0>)\n"}],"source":"import torch\n\ninput = torch.randn(784)\nmlp = MLP()\noutput = mlp(input)\nprint(output)"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Sequential(\n  (0): Linear(in_features=784, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=10, bias=True)\n)\ntensor([-0.1167,  0.0088,  0.3210,  0.1750,  0.2873,  0.2245,  0.0148,  0.0309,\n        -0.1957, -0.2048], grad_fn=<AddBackward0>)\n"}],"source":"# Module的子类：Module类是一个通用的部件，事实上，PyTorch还实现了继承自Module的可以方便构建模型的类：Sequential、ModuleList、ModuleDict\n# Sequential类的目的：接受一个子模块的有序字典或者一系列子模块作为参数来逐一添加Module的实例，模型的前向计算就是按顺序逐一计算\nsequentialMLP = nn.Sequential(nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10))\nprint(sequentialMLP)\nsequentialOutput = sequentialMLP(input)\nprint(sequentialOutput)"},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"ModuleList(\n  (0): Linear(in_features=784, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=10, bias=True)\n)\n"}],"source":"# ModuleList接收一个子模块的列表作为输入，然后可以类似List那样进行append和extend操作\nmoduleLstNet = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])\nmoduleLstNet.append(nn.Linear(256, 10))\nprint(moduleLstNet)\n"},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"ModuleDict(\n  (act): ReLU()\n  (linear): Linear(in_features=784, out_features=256, bias=True)\n  (output): Linear(in_features=256, out_features=10, bias=True)\n)\n"}],"source":"# moduleDict接收一个子模块的字典作为输入，然后也可以像字典那样进行添加访问操作\nmoduleDictNet = nn.ModuleDict({'linear': nn.Linear(784, 256), 'act': nn.ReLU()})\nmoduleDictNet['output'] = nn.Linear(256, 10)\nprint(moduleDictNet)"},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":"class FancyMLP(nn.Module):\n    def __init__(self, **kwargs):\n        super(FancyMLP, self).__init__(**kwargs)\n        self.randWeight = torch.rand((20, 20), requires_grad=False)\n        self.linear = nn.Linear(20, 20)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = nn.functional.relu(torch.mm(x, self.randWeight.data) + 1) # 使用了常数参数\n        x = self.linear(x)    # 重复使用了self.linear层\n\n        # python控制流\n        while x.norm().item() > 1: x /= 2\n        if x.norm().item() < 0.8: x *= 10\n\n        return x.sum()"},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor(-3.0820, grad_fn=<SumBackward0>)\n"}],"source":"input = torch.rand(2, 20)\nfancyMLP = FancyMLP()\noutput = fancyMLP(input)\nprint(output)"},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[0.4661, 0.8697, 0.1475, 0.9463, 0.5911, 0.6075, 0.2942, 0.2041, 0.4409,\n         0.2621, 0.2661, 0.4380, 0.3620, 0.6661, 0.2655, 0.4746, 0.6140, 0.5595,\n         0.0487, 0.3103],\n        [0.2772, 0.2375, 0.8919, 0.0775, 0.1416, 0.2169, 0.1617, 0.1309, 0.3342,\n         0.5981, 0.2956, 0.5038, 0.5102, 0.8190, 0.0090, 0.3568, 0.3549, 0.4321,\n         0.8918, 0.9339],\n        [0.5713, 0.9271, 0.8925, 0.4028, 0.3958, 0.2615, 0.3960, 0.2584, 0.5241,\n         0.1695, 0.7487, 0.2654, 0.6945, 0.4166, 0.6598, 0.0292, 0.9648, 0.5710,\n         0.5854, 0.0750],\n        [0.1632, 0.3876, 0.0704, 0.2999, 0.0604, 0.3231, 0.7501, 0.1435, 0.2435,\n         0.0902, 0.6501, 0.1142, 0.6470, 0.6841, 0.0378, 0.9717, 0.0030, 0.7530,\n         0.2392, 0.1436],\n        [0.7349, 0.5020, 0.0594, 0.3663, 0.7038, 0.3981, 0.1894, 0.5450, 0.4313,\n         0.7990, 0.6736, 0.8064, 0.7079, 0.8475, 0.4513, 0.7996, 0.6589, 0.3753,\n         0.7734, 0.0187],\n        [0.4264, 0.0035, 0.4294, 0.9358, 0.0998, 0.3618, 0.0695, 0.7148, 0.0647,\n         0.0079, 0.3528, 0.0138, 0.7600, 0.2025, 0.4377, 0.6025, 0.1312, 0.7036,\n         0.2297, 0.4301],\n        [0.1555, 0.1673, 0.2131, 0.1462, 0.3753, 0.4976, 0.4876, 0.6353, 0.6785,\n         0.4009, 0.8650, 0.5660, 0.0671, 0.2009, 0.5428, 0.6905, 0.7693, 0.4511,\n         0.1381, 0.2256],\n        [0.0929, 0.1333, 0.0529, 0.4946, 0.7597, 0.8219, 0.1637, 0.3766, 0.5857,\n         0.9196, 0.0210, 0.6299, 0.8721, 0.4368, 0.7494, 0.5269, 0.0517, 0.7266,\n         0.0895, 0.2837],\n        [0.1538, 0.0016, 0.8253, 0.1262, 0.1063, 0.9561, 0.3543, 0.1123, 0.2318,\n         0.8031, 0.0506, 0.9160, 0.4827, 0.4699, 0.3637, 0.4575, 0.3342, 0.2192,\n         0.4630, 0.0071],\n        [0.7205, 0.9823, 0.4803, 0.7911, 0.3575, 0.8253, 0.1300, 0.3092, 0.3881,\n         0.1405, 0.5686, 0.4556, 0.0502, 0.0055, 0.8308, 0.3525, 0.4018, 0.0137,\n         0.1518, 0.4422],\n        [0.5343, 0.3078, 0.1648, 0.5959, 0.7520, 0.5042, 0.0707, 0.8753, 0.6384,\n         0.4205, 0.8543, 0.9981, 0.9938, 0.6800, 0.5714, 0.0397, 0.7897, 0.3149,\n         0.4545, 0.3238],\n        [0.4863, 0.1099, 0.8733, 0.7575, 0.2074, 0.4490, 0.1755, 0.8565, 0.4936,\n         0.3175, 0.8909, 0.9380, 0.6045, 0.6793, 0.5964, 0.7297, 0.9671, 0.5045,\n         0.6014, 0.3579],\n        [0.3257, 0.0661, 0.5935, 0.7495, 0.6881, 0.0210, 0.9452, 0.1278, 0.8637,\n         0.9679, 0.4557, 0.2522, 0.8265, 0.9520, 0.1316, 0.2026, 0.3497, 0.9326,\n         0.5375, 0.6289],\n        [0.7070, 0.9760, 0.8283, 0.9147, 0.9468, 0.1991, 0.0939, 0.5958, 0.1319,\n         0.8012, 0.1221, 0.2018, 0.3634, 0.2270, 0.9476, 0.4902, 0.4151, 0.2728,\n         0.1097, 0.7676],\n        [0.7610, 0.2455, 0.4189, 0.9461, 0.7394, 0.0161, 0.5161, 0.1281, 0.9802,\n         0.5596, 0.9324, 0.9776, 0.7981, 0.2499, 0.9700, 0.4327, 0.7465, 0.7457,\n         0.5152, 0.0384],\n        [0.4941, 0.0602, 0.2808, 0.8636, 0.7431, 0.9785, 0.0511, 0.1196, 0.7734,\n         0.3772, 0.0221, 0.2569, 0.0281, 0.0135, 0.2035, 0.8510, 0.7305, 0.9734,\n         0.1818, 0.7358],\n        [0.9679, 0.7568, 0.4341, 0.4739, 0.8052, 0.7363, 0.6656, 0.7797, 0.5138,\n         0.6746, 0.9114, 0.0449, 0.4674, 0.6059, 0.9285, 0.5339, 0.4521, 0.8509,\n         0.3245, 0.9422],\n        [0.2049, 0.4451, 0.8215, 0.1648, 0.9789, 0.0406, 0.4070, 0.4509, 0.0064,\n         0.0817, 0.4244, 0.3740, 0.0291, 0.0502, 0.8523, 0.3871, 0.6680, 0.1465,\n         0.7324, 0.7998],\n        [0.2035, 0.0822, 0.3672, 0.2236, 0.4019, 0.8008, 0.4547, 0.9037, 0.5472,\n         0.3357, 0.1518, 0.9976, 0.3311, 0.1396, 0.0248, 0.4423, 0.3924, 0.9633,\n         0.7384, 0.7979],\n        [0.1008, 0.3338, 0.8103, 0.9632, 0.3719, 0.0039, 0.9689, 0.5588, 0.7921,\n         0.3560, 0.0979, 0.5236, 0.3236, 0.3423, 0.1515, 0.5314, 0.4420, 0.7310,\n         0.2025, 0.7475]])\ntensor([[0.4661, 0.8697, 0.1475, 0.9463, 0.5911, 0.6075, 0.2942, 0.2041, 0.4409,\n         0.2621, 0.2661, 0.4380, 0.3620, 0.6661, 0.2655, 0.4746, 0.6140, 0.5595,\n         0.0487, 0.3103],\n        [0.2772, 0.2375, 0.8919, 0.0775, 0.1416, 0.2169, 0.1617, 0.1309, 0.3342,\n         0.5981, 0.2956, 0.5038, 0.5102, 0.8190, 0.0090, 0.3568, 0.3549, 0.4321,\n         0.8918, 0.9339],\n        [0.5713, 0.9271, 0.8925, 0.4028, 0.3958, 0.2615, 0.3960, 0.2584, 0.5241,\n         0.1695, 0.7487, 0.2654, 0.6945, 0.4166, 0.6598, 0.0292, 0.9648, 0.5710,\n         0.5854, 0.0750],\n        [0.1632, 0.3876, 0.0704, 0.2999, 0.0604, 0.3231, 0.7501, 0.1435, 0.2435,\n         0.0902, 0.6501, 0.1142, 0.6470, 0.6841, 0.0378, 0.9717, 0.0030, 0.7530,\n         0.2392, 0.1436],\n        [0.7349, 0.5020, 0.0594, 0.3663, 0.7038, 0.3981, 0.1894, 0.5450, 0.4313,\n         0.7990, 0.6736, 0.8064, 0.7079, 0.8475, 0.4513, 0.7996, 0.6589, 0.3753,\n         0.7734, 0.0187],\n        [0.4264, 0.0035, 0.4294, 0.9358, 0.0998, 0.3618, 0.0695, 0.7148, 0.0647,\n         0.0079, 0.3528, 0.0138, 0.7600, 0.2025, 0.4377, 0.6025, 0.1312, 0.7036,\n         0.2297, 0.4301],\n        [0.1555, 0.1673, 0.2131, 0.1462, 0.3753, 0.4976, 0.4876, 0.6353, 0.6785,\n         0.4009, 0.8650, 0.5660, 0.0671, 0.2009, 0.5428, 0.6905, 0.7693, 0.4511,\n         0.1381, 0.2256],\n        [0.0929, 0.1333, 0.0529, 0.4946, 0.7597, 0.8219, 0.1637, 0.3766, 0.5857,\n         0.9196, 0.0210, 0.6299, 0.8721, 0.4368, 0.7494, 0.5269, 0.0517, 0.7266,\n         0.0895, 0.2837],\n        [0.1538, 0.0016, 0.8253, 0.1262, 0.1063, 0.9561, 0.3543, 0.1123, 0.2318,\n         0.8031, 0.0506, 0.9160, 0.4827, 0.4699, 0.3637, 0.4575, 0.3342, 0.2192,\n         0.4630, 0.0071],\n        [0.7205, 0.9823, 0.4803, 0.7911, 0.3575, 0.8253, 0.1300, 0.3092, 0.3881,\n         0.1405, 0.5686, 0.4556, 0.0502, 0.0055, 0.8308, 0.3525, 0.4018, 0.0137,\n         0.1518, 0.4422],\n        [0.5343, 0.3078, 0.1648, 0.5959, 0.7520, 0.5042, 0.0707, 0.8753, 0.6384,\n         0.4205, 0.8543, 0.9981, 0.9938, 0.6800, 0.5714, 0.0397, 0.7897, 0.3149,\n         0.4545, 0.3238],\n        [0.4863, 0.1099, 0.8733, 0.7575, 0.2074, 0.4490, 0.1755, 0.8565, 0.4936,\n         0.3175, 0.8909, 0.9380, 0.6045, 0.6793, 0.5964, 0.7297, 0.9671, 0.5045,\n         0.6014, 0.3579],\n        [0.3257, 0.0661, 0.5935, 0.7495, 0.6881, 0.0210, 0.9452, 0.1278, 0.8637,\n         0.9679, 0.4557, 0.2522, 0.8265, 0.9520, 0.1316, 0.2026, 0.3497, 0.9326,\n         0.5375, 0.6289],\n        [0.7070, 0.9760, 0.8283, 0.9147, 0.9468, 0.1991, 0.0939, 0.5958, 0.1319,\n         0.8012, 0.1221, 0.2018, 0.3634, 0.2270, 0.9476, 0.4902, 0.4151, 0.2728,\n         0.1097, 0.7676],\n        [0.7610, 0.2455, 0.4189, 0.9461, 0.7394, 0.0161, 0.5161, 0.1281, 0.9802,\n         0.5596, 0.9324, 0.9776, 0.7981, 0.2499, 0.9700, 0.4327, 0.7465, 0.7457,\n         0.5152, 0.0384],\n        [0.4941, 0.0602, 0.2808, 0.8636, 0.7431, 0.9785, 0.0511, 0.1196, 0.7734,\n         0.3772, 0.0221, 0.2569, 0.0281, 0.0135, 0.2035, 0.8510, 0.7305, 0.9734,\n         0.1818, 0.7358],\n        [0.9679, 0.7568, 0.4341, 0.4739, 0.8052, 0.7363, 0.6656, 0.7797, 0.5138,\n         0.6746, 0.9114, 0.0449, 0.4674, 0.6059, 0.9285, 0.5339, 0.4521, 0.8509,\n         0.3245, 0.9422],\n        [0.2049, 0.4451, 0.8215, 0.1648, 0.9789, 0.0406, 0.4070, 0.4509, 0.0064,\n         0.0817, 0.4244, 0.3740, 0.0291, 0.0502, 0.8523, 0.3871, 0.6680, 0.1465,\n         0.7324, 0.7998],\n        [0.2035, 0.0822, 0.3672, 0.2236, 0.4019, 0.8008, 0.4547, 0.9037, 0.5472,\n         0.3357, 0.1518, 0.9976, 0.3311, 0.1396, 0.0248, 0.4423, 0.3924, 0.9633,\n         0.7384, 0.7979],\n        [0.1008, 0.3338, 0.8103, 0.9632, 0.3719, 0.0039, 0.9689, 0.5588, 0.7921,\n         0.3560, 0.0979, 0.5236, 0.3236, 0.3423, 0.1515, 0.5314, 0.4420, 0.7310,\n         0.2025, 0.7475]])\n"}],"source":"print(fancyMLP.randWeight)\noutput.backward()\nprint(fancyMLP.randWeight)"},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor(2.6561, grad_fn=<SumBackward0>)\n"}],"source":"# 模型嵌套调用\n\nclass NestMLP(nn.Module):\n    def __init__(self, **kwargs):\n        super(NestMLP, self).__init__(**kwargs)\n        self.net = nn.Sequential(nn.Linear(40, 30), nn.ReLU())\n    \n    def forward(self, x):\n        return self.net(x)\n\nnestModule = nn.Sequential(NestMLP(), nn.Linear(30, 20), FancyMLP())\ninput = torch.rand(2, 40)\nprint(nestModule(input))"},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"<class 'generator'>\nParameter containing:\ntensor([[ 1.0632e-01, -1.0758e-02,  1.1995e-04,  ...,  4.5790e-02,\n          6.8271e-02,  9.8536e-02],\n        [-4.1305e-02,  2.6749e-02,  3.3654e-02,  ...,  8.4781e-02,\n         -9.0875e-03, -5.2351e-02],\n        [-1.3303e-01,  5.5016e-02, -1.2562e-01,  ...,  1.0229e-01,\n         -2.8073e-02,  3.4376e-02],\n        ...,\n        [-1.0138e-01,  3.5060e-02,  5.2653e-02,  ..., -1.0547e-01,\n          2.3706e-02, -1.2087e-02],\n        [-1.3556e-01, -4.0240e-02,  3.3253e-02,  ...,  8.6094e-02,\n          1.5793e-01, -7.9311e-02],\n        [-9.8206e-02,  1.5464e-01, -5.9911e-02,  ..., -1.0440e-01,\n          1.3490e-01,  1.2286e-01]], requires_grad=True)\nParameter containing:\ntensor([-1.2516e-01, -6.8050e-02,  1.2106e-01,  1.0394e-01,  3.0253e-02,\n        -8.7294e-02, -1.1264e-04,  5.2160e-02,  5.3115e-02,  1.5100e-01,\n        -1.4198e-01, -1.5284e-01, -1.0123e-01,  1.4491e-01,  5.6987e-02,\n        -1.5461e-01,  2.6199e-02,  1.3271e-01,  1.3999e-01, -1.2825e-01,\n         4.2453e-05, -1.1982e-02,  3.1981e-02, -1.2815e-01,  1.4334e-01,\n         5.1772e-02, -3.9826e-03,  1.8624e-02,  1.2759e-01, -1.1282e-01],\n       requires_grad=True)\nParameter containing:\ntensor([[-1.2515e-01, -2.4020e-02, -2.3627e-02, -1.6608e-01, -9.8981e-02,\n         -5.4313e-02, -1.1680e-01, -7.0029e-02,  3.8898e-02, -1.0222e-01,\n         -1.3306e-01,  5.9744e-02, -8.7484e-03,  1.2016e-01, -1.2626e-01,\n          1.7336e-03,  1.7188e-01,  1.2124e-01, -3.2741e-02,  1.0488e-01,\n         -1.4870e-01,  1.3300e-01, -5.4010e-02, -1.4953e-01, -1.2015e-01,\n          3.6135e-02, -1.2261e-01, -1.4841e-01, -1.3565e-02,  5.1535e-02],\n        [-1.0312e-01, -1.0465e-01,  1.7933e-01,  5.0435e-02,  3.3134e-02,\n          3.6014e-02,  1.5924e-01, -6.5486e-02, -1.1513e-01,  1.5019e-01,\n         -3.2105e-02, -2.8055e-02,  1.5557e-01,  1.5551e-01,  1.0902e-01,\n          4.2986e-02,  1.0891e-01, -1.1258e-01, -5.8903e-02, -6.1205e-02,\n         -5.5002e-04, -7.4580e-02, -1.2655e-01,  1.3923e-01, -1.0018e-02,\n          1.1663e-01, -1.4125e-02, -2.1032e-02,  1.4424e-01,  1.3129e-01],\n        [-1.5767e-01,  3.3995e-02,  9.1138e-02,  1.6302e-01,  7.6295e-02,\n          1.8192e-01, -8.2942e-02,  1.0280e-01, -1.1423e-02, -4.9717e-02,\n          7.6898e-02, -1.1459e-01, -6.2920e-02,  1.5946e-02, -4.1840e-03,\n         -5.1754e-02, -1.3896e-01, -1.0627e-01,  1.2766e-01,  8.1779e-02,\n         -7.8228e-03,  9.6502e-02,  1.5668e-01, -1.4164e-01, -1.0085e-01,\n          1.3293e-01, -2.9240e-02,  1.4121e-01, -1.2989e-02,  1.5444e-01],\n        [ 3.1307e-02, -1.7369e-01,  1.8235e-01, -5.2021e-02,  1.0752e-02,\n          1.3592e-01, -9.6018e-02, -1.3939e-01, -8.3811e-02, -1.2124e-01,\n          1.5888e-01, -5.0618e-02, -7.7058e-02,  9.5623e-02,  4.3013e-02,\n          1.2470e-01,  7.4865e-02,  1.5005e-01, -1.7518e-01, -1.9999e-02,\n         -1.7237e-01, -6.0584e-02, -1.5354e-01, -1.9936e-02,  1.8248e-01,\n         -2.4262e-02,  1.2819e-01, -3.3749e-02, -9.8346e-02, -1.0943e-01],\n        [-1.7212e-01, -1.6580e-01,  7.0001e-03, -9.5422e-02, -1.5086e-01,\n         -1.8153e-01, -8.0844e-02, -2.4458e-02, -5.9721e-02, -1.7311e-01,\n          1.4164e-01,  1.1312e-01,  1.2861e-01, -8.7829e-02,  1.3366e-01,\n          1.4225e-01,  5.5261e-02, -1.8102e-01, -3.1011e-02,  1.4684e-01,\n         -5.7931e-02,  7.0093e-02, -8.3096e-02, -2.3773e-02, -8.8068e-02,\n          7.0705e-02,  9.8240e-02, -6.9622e-02,  7.3883e-02,  8.1745e-02],\n        [-1.2381e-01, -3.6750e-02, -9.4274e-03,  1.8196e-01,  5.8547e-02,\n          1.6658e-01,  4.3779e-02, -1.5914e-01,  1.5668e-01, -4.9002e-02,\n         -3.6712e-02, -5.4585e-02, -4.5854e-02,  1.4723e-01, -1.2223e-01,\n          1.6817e-01,  1.3725e-01, -1.5702e-01,  1.7753e-01,  3.4854e-02,\n          1.6367e-01, -5.5042e-02, -7.0810e-02, -2.0207e-02, -1.7028e-02,\n          1.7239e-01, -9.2194e-02,  8.2708e-02,  4.7283e-04,  1.4190e-01],\n        [ 1.2432e-01, -1.7667e-02, -5.6606e-02, -1.3244e-01, -1.7968e-01,\n         -3.5240e-02,  1.6472e-01,  4.6441e-02, -1.9766e-02, -7.0357e-03,\n         -8.5738e-02, -4.3056e-02,  1.4385e-01,  2.2582e-02, -1.1117e-01,\n          1.0868e-01,  1.2915e-01,  9.0096e-02,  4.7739e-02,  9.1345e-02,\n          5.8762e-02, -2.7659e-02, -1.5918e-01, -6.1226e-02, -1.2275e-01,\n         -1.7212e-01,  1.5885e-02,  1.0639e-01,  9.2419e-02, -6.9368e-02],\n        [-1.0222e-01, -1.1354e-02, -1.2174e-01, -1.0612e-01,  1.0499e-01,\n         -1.2098e-01, -9.9739e-02,  9.8971e-02, -1.1743e-01,  1.5179e-01,\n         -3.3783e-02, -9.1342e-02,  1.6893e-01, -8.4850e-02,  1.9957e-02,\n         -1.3491e-01, -1.7776e-01,  2.0952e-02,  4.6636e-02,  1.0382e-01,\n          2.1380e-02, -1.3845e-03,  6.1076e-02,  1.2641e-01,  1.6548e-01,\n          2.9246e-02, -1.0095e-01,  5.2848e-02,  8.0252e-02,  4.0923e-02],\n        [ 1.3642e-01,  6.9446e-02,  3.1097e-02, -1.5093e-01,  1.6826e-01,\n         -1.5460e-01, -1.6470e-01, -1.5294e-01,  1.5069e-01, -7.5182e-02,\n          6.9355e-02,  6.0073e-02, -6.8091e-03,  1.6529e-01,  5.7269e-02,\n          1.0856e-01,  1.6005e-01, -1.6346e-02, -7.4376e-03, -1.7813e-01,\n          8.1972e-02, -1.0738e-01, -1.7900e-01, -1.7058e-01,  7.3417e-02,\n         -1.3372e-01, -1.8095e-01,  1.0388e-01, -1.1225e-01, -1.2420e-01],\n        [ 1.6196e-01,  1.6104e-01, -7.3079e-02, -1.0709e-01,  1.3160e-01,\n         -1.5157e-01,  5.7659e-02, -1.6503e-01,  5.0455e-03,  1.2935e-01,\n         -1.4987e-01, -7.2330e-02,  1.1473e-01,  1.3352e-01, -8.8605e-02,\n          5.2569e-02,  1.4756e-02,  2.6258e-02,  3.4199e-02,  2.1890e-02,\n          9.6921e-02,  9.0605e-02,  1.7561e-01,  1.0150e-01, -1.4660e-01,\n          1.0737e-01, -1.7592e-01, -4.2173e-03,  1.4566e-02,  1.3885e-01],\n        [ 2.0246e-02,  1.1709e-01, -1.6525e-01,  2.6698e-02, -8.6680e-02,\n         -1.4521e-01, -1.1986e-01, -4.0154e-02,  3.9282e-02, -6.0664e-02,\n          1.8197e-01, -1.3484e-01, -6.2834e-02, -1.5285e-01, -2.2355e-02,\n         -3.3510e-02,  4.8330e-02,  6.9058e-02,  4.8836e-02,  1.0664e-01,\n         -5.8588e-02, -1.9192e-03, -6.4046e-02,  1.2353e-01,  7.4408e-02,\n          8.4870e-02,  3.8687e-02,  5.2879e-02,  1.2850e-01, -1.7637e-01],\n        [ 1.4729e-01, -1.1150e-01,  1.1131e-01,  1.9912e-04, -2.6543e-02,\n          1.6100e-01, -4.2051e-02, -7.8311e-02,  1.5298e-01,  4.5534e-02,\n         -1.6870e-01, -2.4797e-03, -1.5040e-01,  1.7608e-01, -3.7063e-02,\n         -1.1691e-01,  9.4287e-02, -1.2379e-01,  5.2594e-02, -1.8187e-01,\n          4.4175e-02,  5.8755e-02, -1.4357e-01,  9.0620e-03,  8.0312e-02,\n          1.4138e-01,  1.9571e-02, -1.4010e-01, -1.6153e-01,  5.0279e-03],\n        [-1.4792e-01, -1.8013e-01, -2.4931e-02, -7.2780e-02, -1.8240e-01,\n          7.6518e-02,  8.5857e-02, -1.0407e-01,  1.0847e-01, -1.0767e-01,\n          1.2174e-02, -5.1064e-03, -1.2746e-03,  3.7389e-02,  8.2135e-02,\n          1.2927e-01, -9.6211e-03, -1.5864e-01, -1.0740e-01, -5.7809e-02,\n         -1.4489e-01, -9.7169e-02, -1.0647e-01, -1.4393e-01, -1.7783e-01,\n          1.7903e-01, -7.7134e-02, -5.4854e-02,  2.9753e-02,  6.1530e-02],\n        [-8.4403e-02,  2.6391e-02, -1.3929e-01,  1.4298e-02,  8.7625e-02,\n          3.5675e-02,  1.2761e-01, -8.9827e-02, -8.8817e-02,  5.5087e-02,\n          1.0730e-01,  7.3619e-02, -8.7351e-02, -1.7969e-01,  1.4681e-01,\n          1.2040e-01, -7.6835e-02, -8.1952e-02,  1.7799e-01, -1.2512e-01,\n         -1.7137e-01,  6.3386e-02, -1.7558e-01, -1.5929e-05, -1.6121e-01,\n         -1.3363e-01,  7.2027e-02, -6.8106e-02,  1.6333e-01, -4.6829e-02],\n        [-1.7660e-01, -1.4147e-01, -1.1571e-01,  1.7588e-01, -1.3098e-01,\n          1.4316e-01, -2.6263e-02,  1.5615e-01, -2.6721e-02,  8.6524e-02,\n         -2.1892e-03,  5.9098e-02, -3.6114e-02,  1.7252e-01, -1.4408e-01,\n         -1.0982e-01,  4.4133e-02, -1.0852e-01, -9.2512e-02,  1.0168e-01,\n          5.7977e-02, -1.4213e-01,  8.6060e-02, -1.4982e-01, -1.7886e-02,\n          4.0560e-02,  1.7961e-01, -2.7814e-02, -1.6236e-01,  9.7177e-03],\n        [-1.0189e-01,  9.5315e-02,  1.6988e-01, -7.5854e-02, -5.3585e-02,\n          5.1670e-02, -1.2073e-01, -1.0970e-01, -1.6686e-02, -1.0301e-01,\n         -9.6790e-02,  5.1664e-02,  1.7140e-01,  4.7222e-02,  1.7927e-01,\n          7.1247e-02, -1.5646e-01,  2.7268e-02, -6.9160e-02, -2.1314e-02,\n          4.1721e-02,  1.7972e-01,  1.1055e-01,  1.2379e-01,  1.7964e-01,\n         -1.7523e-01, -9.3815e-02,  1.3154e-01, -2.3104e-03, -2.5756e-02],\n        [ 7.7161e-02,  1.1146e-01, -1.6193e-02, -1.3072e-01,  1.7016e-01,\n          9.9623e-02, -3.4223e-02, -1.0999e-01, -9.4282e-02,  7.5849e-02,\n         -1.6760e-01,  7.3642e-02,  1.1086e-01,  7.5642e-02,  1.8056e-01,\n          5.7847e-02,  1.3165e-01,  7.8546e-02, -1.7222e-01,  1.7671e-01,\n          1.2592e-01,  4.9377e-02, -3.4192e-03, -1.1789e-01,  9.2778e-02,\n         -1.7341e-01,  6.0522e-02,  2.5733e-02, -8.8236e-02, -6.0263e-02],\n        [-3.8687e-02,  6.2556e-02, -3.6135e-02,  1.4990e-01,  1.7597e-01,\n         -7.2041e-02, -1.3971e-01, -1.8137e-01,  1.6321e-01, -1.7182e-01,\n          1.3893e-02,  9.6201e-03, -8.2775e-02,  6.5155e-03,  9.1794e-02,\n         -4.1676e-02,  4.7946e-02, -1.0661e-01,  1.5245e-01,  1.0753e-01,\n          1.8117e-01, -7.7696e-02, -2.1997e-02, -8.0214e-02, -2.5948e-02,\n          1.3450e-01,  1.4346e-01, -2.0562e-02, -3.1205e-02,  5.0380e-02],\n        [ 1.2325e-01, -1.0637e-02,  6.2017e-02,  6.0352e-02,  2.8343e-02,\n         -7.5130e-02,  1.1320e-01,  7.6454e-02,  1.1305e-01,  1.5496e-03,\n         -1.2029e-01,  1.5436e-02, -6.0757e-03,  5.8808e-02, -3.3787e-03,\n         -7.3796e-02,  4.9039e-02,  1.5288e-01, -6.9261e-02, -1.2567e-01,\n          1.2209e-01, -6.1374e-03, -1.0717e-01,  4.1728e-02,  1.4917e-01,\n          1.0823e-01, -9.2339e-02, -1.8237e-02,  6.3866e-02, -5.6616e-02],\n        [ 1.1022e-01, -1.4489e-01,  4.7113e-02,  2.4799e-02,  1.4330e-01,\n          6.8857e-02,  3.1502e-02,  1.0763e-03, -5.4375e-02, -1.4047e-01,\n         -1.2333e-02, -3.8296e-02,  9.6060e-02,  1.6990e-01, -1.7230e-01,\n          1.2303e-01, -1.5825e-01,  3.8439e-02,  8.6612e-02, -1.4378e-01,\n          5.1676e-02, -5.5888e-02, -1.4581e-01,  7.2416e-02,  1.2425e-01,\n          3.7314e-02, -4.9560e-03, -1.2940e-01,  4.3015e-02, -1.1730e-01]],\n       requires_grad=True)\nParameter containing:\ntensor([ 0.1599,  0.0613,  0.1751, -0.0901,  0.1337,  0.0099, -0.0154,  0.0263,\n        -0.1514,  0.1707, -0.0202, -0.1167, -0.0279, -0.1266, -0.0959, -0.0522,\n         0.0477, -0.1106, -0.0326, -0.1378], requires_grad=True)\nParameter containing:\ntensor([[ 0.1702,  0.1929, -0.1040, -0.2132, -0.1393,  0.1174, -0.0829,  0.1574,\n          0.2136,  0.2058,  0.0243,  0.2058, -0.1129, -0.1636, -0.1698,  0.0984,\n          0.1880,  0.0126, -0.1660, -0.0625],\n        [ 0.0650, -0.2083, -0.0722,  0.2111, -0.1997,  0.1344,  0.1314, -0.0826,\n         -0.0585, -0.1238, -0.1136,  0.0214,  0.1735,  0.0045, -0.1973, -0.1099,\n         -0.0185, -0.1277, -0.1268,  0.2203],\n        [-0.0974, -0.0750, -0.2226,  0.1541, -0.1004,  0.1939,  0.2073, -0.1167,\n          0.1540,  0.2010, -0.0944, -0.1137, -0.1854,  0.0639,  0.0187,  0.1601,\n         -0.0118,  0.1120, -0.1507, -0.1309],\n        [ 0.0011, -0.0600,  0.2196,  0.2121,  0.2023,  0.2021,  0.1286,  0.0571,\n          0.1264, -0.0199, -0.1734, -0.1965,  0.1205,  0.1392, -0.2088, -0.1523,\n         -0.1936,  0.0612,  0.2099, -0.0161],\n        [ 0.0244, -0.1583,  0.0625, -0.1618, -0.1834,  0.0987,  0.0559,  0.0562,\n         -0.0579, -0.2085, -0.1249, -0.2174,  0.2179,  0.1353,  0.0922,  0.0883,\n         -0.1155,  0.1820,  0.0555,  0.1113],\n        [-0.0364, -0.0464, -0.1009, -0.2222, -0.0499, -0.1516, -0.0797, -0.0612,\n          0.1359,  0.1697,  0.0844,  0.0273,  0.1615,  0.0018,  0.1136, -0.0027,\n          0.1187, -0.0593,  0.2068,  0.1152],\n        [-0.0494,  0.0302,  0.0859,  0.1292, -0.0121,  0.0920, -0.0604,  0.1883,\n          0.1266,  0.1582, -0.0119, -0.0946,  0.0018,  0.2207,  0.0601,  0.1655,\n          0.1507, -0.1005,  0.0415,  0.0294],\n        [ 0.1272,  0.1836, -0.1694,  0.0184, -0.1774,  0.0372,  0.1212,  0.0553,\n          0.1325,  0.2233,  0.0286, -0.0009, -0.1493, -0.2202, -0.2018,  0.1615,\n         -0.2027, -0.0870,  0.1817, -0.0411],\n        [-0.0279, -0.2193,  0.1262,  0.1814, -0.1986,  0.1593, -0.0285, -0.1210,\n          0.2060, -0.0058, -0.1157,  0.1400, -0.1018,  0.1800, -0.1262, -0.1899,\n          0.0136,  0.0884, -0.1462, -0.1851],\n        [ 0.1335, -0.1039, -0.1651, -0.0810, -0.0134, -0.0402,  0.0508, -0.0354,\n         -0.0183, -0.0312,  0.0056, -0.1975,  0.1426,  0.1098,  0.2026,  0.0012,\n          0.1510,  0.1067,  0.2125, -0.1039],\n        [-0.1780, -0.0189, -0.1245,  0.0294,  0.0525, -0.2190,  0.0007,  0.2200,\n          0.2166,  0.1369,  0.1815, -0.0755, -0.1068, -0.1278,  0.0779, -0.2155,\n          0.0218,  0.0391, -0.1589,  0.0938],\n        [ 0.1732,  0.0536,  0.1241,  0.1796, -0.2048, -0.1429,  0.1368, -0.1593,\n         -0.1392, -0.2222,  0.1186,  0.1101,  0.1538, -0.0400,  0.1183,  0.1419,\n          0.1738,  0.0529, -0.1673, -0.1098],\n        [-0.0710,  0.1411,  0.1240,  0.1437,  0.0825,  0.0680,  0.0687, -0.1871,\n          0.0436, -0.1788, -0.0203,  0.0802,  0.1916, -0.0834,  0.1541, -0.1053,\n         -0.0939,  0.2119, -0.0462,  0.0523],\n        [ 0.1720, -0.2198,  0.2138, -0.1624,  0.0325, -0.2089, -0.0589,  0.1689,\n         -0.1885,  0.0407,  0.2199, -0.0938, -0.1253, -0.1677, -0.1093, -0.0337,\n         -0.2013,  0.1729, -0.1860,  0.1559],\n        [-0.1585, -0.1501,  0.1933, -0.0909, -0.1257,  0.2156,  0.1089,  0.1546,\n          0.0807,  0.1880, -0.1492, -0.0779,  0.0333,  0.1057,  0.0625, -0.0010,\n         -0.1729, -0.2177,  0.0027, -0.0171],\n        [ 0.0835,  0.1479,  0.0390, -0.1101, -0.0271,  0.1341,  0.0306,  0.0092,\n         -0.2002,  0.0002,  0.0841, -0.0110, -0.1437,  0.0328,  0.0411, -0.0389,\n          0.1806, -0.1942, -0.0097, -0.0145],\n        [ 0.0487, -0.1312, -0.1022, -0.1527,  0.1921, -0.0403,  0.1826,  0.0605,\n         -0.2144,  0.2037, -0.1181,  0.0762,  0.2170,  0.1410, -0.1137, -0.0597,\n         -0.1845,  0.0388,  0.0979,  0.1407],\n        [-0.1031, -0.1116, -0.0852, -0.0159, -0.1913, -0.0940, -0.2004,  0.1478,\n          0.0779, -0.0763, -0.1954, -0.0084,  0.2003,  0.2025, -0.0723,  0.0858,\n          0.0926,  0.1472,  0.0476, -0.1515],\n        [ 0.0056,  0.1280,  0.1102, -0.0485, -0.1287,  0.0147, -0.0919, -0.1268,\n          0.1659, -0.1805, -0.1056, -0.0333,  0.1787,  0.1692,  0.1088,  0.0044,\n         -0.1205,  0.0447, -0.2167, -0.0884],\n        [ 0.0452, -0.1554, -0.2235,  0.1035, -0.0841,  0.0460, -0.0925, -0.0851,\n         -0.1467,  0.1772, -0.2005, -0.0520,  0.0028, -0.0632, -0.0729, -0.0094,\n         -0.0555, -0.1485, -0.2097,  0.1862]], requires_grad=True)\nParameter containing:\ntensor([ 0.1261,  0.1083, -0.0935,  0.1836, -0.0686, -0.1623, -0.0069,  0.1708,\n         0.1949,  0.0454, -0.0429,  0.1741,  0.0157, -0.1517, -0.1570, -0.1876,\n        -0.1095,  0.1518, -0.1415, -0.2047], requires_grad=True)\n"}],"source":"# 访问模型参数 parameters方式\nprint(type(nestModule.parameters()))\nfor param in nestModule.parameters(): print(param)"},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"<class 'generator'>\n0.net.0.weight\n0.net.0.bias\n1.weight\n1.bias\n2.linear.weight\n2.linear.bias\n\n\nweight\nbias\n"}],"source":"# 访问模型参数 named_parameters方式\nprint(type(nestModule.named_parameters()))\nfor name, param in nestModule.named_parameters(): print(name)\n# 返回的名字自动加上了层数的索引作为前缀，单层的就没有层数索引的前缀\n#  torch.nn.parameter.Parameter ，其实这是 Tensor 的子类\n\n# randweight不在参数列表中？\n\nprint('\\n')\n# 访问单层的参数\nfor name, param in nestModule[1].named_parameters(): print(name) "},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0.net.0.weighttensor([[-4.4612e-03, -9.7813e-03,  1.6284e-02,  ...,  5.6344e-03,\n         -1.1865e-03, -1.6980e-02],\n        [ 1.8055e-02, -9.5757e-03,  5.2357e-03,  ...,  3.5864e-03,\n         -2.4506e-03, -3.8248e-03],\n        [-1.8526e-02, -5.3761e-03, -2.0859e-03,  ...,  9.7163e-03,\n         -3.0434e-05, -3.6126e-03],\n        ...,\n        [ 4.1569e-03,  1.2405e-02,  6.5253e-04,  ...,  1.4102e-02,\n         -9.1181e-04, -1.6776e-02],\n        [-6.7079e-03,  1.4770e-02, -1.1613e-02,  ..., -3.4333e-03,\n          3.4166e-03,  9.4195e-04],\n        [ 2.3919e-03, -1.1134e-02, -2.8177e-03,  ...,  8.7555e-03,\n          3.2819e-03, -8.2593e-03]])\n1.weighttensor([[-1.7538e-03,  1.4744e-02, -4.6096e-03,  2.9317e-03,  1.5924e-03,\n         -7.1981e-03, -6.9165e-03, -1.2513e-02, -2.6573e-03,  7.5224e-03,\n         -9.8681e-03,  1.1862e-02, -1.3333e-02,  6.5121e-03,  3.1250e-03,\n         -4.8340e-03,  1.4213e-03,  1.1448e-02, -7.2540e-03, -1.0928e-02,\n         -1.3325e-02, -9.8972e-03, -6.1179e-03, -2.7947e-03,  1.1985e-02,\n          6.6985e-03, -1.4579e-02,  1.1456e-02,  3.1012e-03, -1.5149e-02],\n        [-1.9860e-02,  2.0145e-02,  6.4233e-03,  1.7905e-03, -9.6708e-03,\n         -2.2593e-03, -6.7657e-03,  1.8823e-03, -8.6125e-03, -4.8184e-03,\n         -1.9869e-02,  2.0337e-02, -9.4051e-03,  8.9596e-03,  4.7560e-03,\n          1.4941e-02,  2.3199e-02, -7.2372e-03, -5.1215e-03,  2.4131e-02,\n         -1.7042e-03,  2.6224e-03, -5.5556e-03,  1.3627e-02,  6.1230e-03,\n         -1.3543e-02,  2.1979e-02, -1.2768e-02,  8.7805e-03,  5.2506e-03],\n        [ 7.8883e-03,  2.7597e-03, -8.8190e-03,  1.1772e-03, -1.7884e-02,\n          1.8662e-03,  1.5966e-02, -1.3097e-02,  2.3559e-02,  2.0665e-02,\n          8.8549e-03,  1.4738e-04, -2.1112e-03,  3.1048e-04,  2.0334e-03,\n         -1.2738e-02, -9.5602e-03, -5.5122e-03,  1.9613e-03, -2.2772e-04,\n          1.6226e-02, -1.3155e-02, -6.6640e-03,  7.3684e-03,  7.0817e-03,\n          8.7556e-03, -8.7404e-03, -6.3392e-03, -1.8265e-02,  1.8936e-02],\n        [ 1.4600e-02,  6.3482e-03,  7.4149e-03, -2.1336e-02, -1.7294e-02,\n          1.3932e-02,  3.4203e-03, -1.3624e-03,  1.0290e-02,  1.3641e-02,\n         -2.2881e-02, -2.1292e-03,  5.9821e-03, -6.5991e-03, -1.4032e-02,\n          7.1307e-03, -1.3952e-02,  4.6036e-03, -8.3640e-04,  9.2097e-03,\n         -1.3436e-02, -2.9530e-03,  1.6795e-03,  8.1020e-03,  1.4567e-02,\n         -1.2742e-02,  5.7243e-03,  2.6609e-04, -1.1001e-02,  8.8457e-04],\n        [ 3.6269e-03, -2.2034e-03, -2.7929e-02, -1.2984e-02, -3.8211e-03,\n          2.9832e-03,  4.9447e-03, -4.9144e-03,  2.8001e-02,  1.6749e-02,\n         -2.6713e-02,  9.8645e-03,  1.0291e-02,  7.8870e-03, -7.9914e-03,\n          1.8705e-03,  6.4648e-03,  1.2403e-02,  7.6532e-03,  3.5107e-03,\n          1.3937e-02, -1.2740e-02,  1.0090e-02,  1.6864e-03, -1.0273e-02,\n         -8.5365e-03, -2.7831e-02,  6.8189e-03,  4.5482e-03, -4.9179e-03],\n        [ 8.5557e-03, -1.1550e-02,  1.2133e-02, -2.0661e-02,  9.3158e-05,\n          1.3124e-02, -1.6975e-03, -6.6659e-03,  6.5921e-03,  1.8907e-04,\n         -6.1955e-03,  3.2381e-03, -1.2367e-03, -2.9352e-03,  1.9427e-02,\n         -1.3671e-02,  7.4668e-03,  1.3222e-02, -6.6313e-03,  1.5034e-03,\n         -1.0051e-02,  2.6573e-03, -1.1444e-02,  1.1012e-02, -1.1292e-02,\n          4.0064e-03, -2.8896e-03, -6.7514e-03,  1.4440e-02, -8.4303e-03],\n        [-2.7711e-03, -1.1343e-02, -5.6592e-03,  1.3031e-02, -8.0508e-03,\n         -1.7284e-02,  1.0818e-02,  1.7595e-03, -2.3384e-02, -1.5230e-03,\n         -6.2867e-03,  1.7759e-03, -2.0372e-03,  5.3646e-03, -4.7080e-03,\n         -3.4592e-03, -9.4372e-03, -1.1292e-02, -1.2717e-02, -8.1452e-03,\n          3.6237e-03,  4.1870e-03, -1.1985e-02, -5.3065e-03, -2.0663e-02,\n         -2.2567e-02,  1.2426e-02,  1.3244e-02, -1.5291e-03, -1.2126e-02],\n        [-2.8139e-03, -1.4962e-02, -5.4114e-03,  1.2159e-02,  1.0364e-02,\n          1.2059e-02, -8.5304e-03,  6.3010e-03,  3.4875e-03, -3.1182e-03,\n          9.3108e-03, -2.5675e-03, -1.2312e-02, -3.8592e-03,  7.7708e-03,\n         -1.7828e-02, -3.7736e-03,  1.0264e-02, -3.0020e-03, -9.1556e-03,\n          1.1740e-02,  6.7469e-03, -5.9266e-03,  2.8746e-03, -4.1528e-03,\n          1.4198e-03, -6.2179e-04,  7.8372e-04, -7.4368e-03,  9.4845e-03],\n        [-9.6395e-03, -1.6015e-03, -6.0461e-03, -9.7367e-03, -6.5557e-03,\n         -2.7231e-03, -1.0050e-03, -1.0510e-02,  1.1576e-02, -4.6938e-04,\n          1.1040e-02,  1.4456e-02,  1.9520e-02, -7.2946e-03, -1.1933e-03,\n         -8.7876e-03, -2.7076e-03,  7.3802e-04, -4.2757e-03, -2.9374e-03,\n         -7.1385e-04,  1.6616e-02,  1.0939e-02, -3.6820e-03,  3.3767e-03,\n          3.5707e-03,  8.2773e-03, -6.9015e-03, -1.2476e-02,  3.3852e-03],\n        [-6.1693e-03,  2.3404e-03, -1.7174e-02, -7.4170e-03, -3.0448e-03,\n         -1.1683e-04,  9.5977e-04, -7.4964e-03, -2.9552e-03, -8.1604e-03,\n         -4.1226e-03,  4.2298e-03,  1.9395e-03, -1.2681e-03,  1.6550e-02,\n          5.2549e-04, -8.2558e-03, -1.8297e-03, -9.5412e-03, -2.8767e-03,\n          9.4271e-03, -8.4181e-03, -1.4832e-02,  9.7294e-03, -2.0157e-03,\n         -8.3797e-03,  1.2238e-02,  1.2170e-02,  2.8324e-03,  2.8908e-03],\n        [ 5.1294e-03, -8.7392e-03,  5.1770e-04,  1.4229e-02,  1.3060e-02,\n          1.8558e-02,  1.1942e-02, -1.4100e-03,  8.1437e-05, -1.7400e-02,\n          2.0524e-02,  9.6768e-03, -1.6429e-03,  2.5170e-03, -5.6656e-03,\n          1.5866e-02,  4.4024e-03, -1.6789e-04, -9.9345e-03,  4.1767e-03,\n          8.0535e-03,  1.3495e-02,  1.1874e-02, -4.2013e-04, -8.9899e-03,\n          8.8914e-03, -2.1455e-03, -3.5621e-03,  8.9852e-03,  2.0811e-03],\n        [ 1.5335e-02, -9.7316e-03, -7.3717e-03,  2.1828e-02, -1.0873e-02,\n         -3.7058e-03, -4.4219e-03, -1.9800e-02,  2.0668e-02, -7.2400e-04,\n          1.1101e-02,  7.5219e-03,  2.0232e-02,  1.0283e-03, -4.6662e-03,\n          6.2671e-03, -1.6519e-02, -8.2719e-03,  2.8871e-03,  3.3175e-03,\n         -1.7277e-02,  1.8513e-02,  6.0487e-03, -1.7089e-02, -5.2231e-03,\n          3.7888e-03, -1.7240e-02, -7.9741e-03, -4.5410e-03,  1.3663e-02],\n        [-2.2463e-03,  9.9390e-03, -6.3567e-03, -3.7192e-03, -2.9069e-04,\n          8.4598e-03,  2.2334e-03, -1.4030e-02, -7.7985e-03,  1.2711e-02,\n          8.5933e-03, -3.5995e-03, -3.3704e-03, -1.0396e-03,  4.4066e-03,\n          7.6317e-04, -7.4644e-03, -2.8504e-03,  3.9933e-03, -1.7832e-02,\n         -7.3952e-03,  1.4385e-02,  7.5672e-03, -1.5622e-03, -1.1226e-02,\n         -9.3060e-04, -2.0439e-02,  8.1809e-03,  1.0624e-02,  1.1559e-02],\n        [-1.8502e-02, -5.4634e-03, -1.0831e-02,  1.7605e-02, -4.0990e-03,\n          7.6869e-03,  2.4656e-03,  2.1218e-02,  7.7524e-03,  1.9274e-02,\n          5.1642e-03, -1.1998e-02,  5.0763e-03, -4.3324e-03,  1.5283e-02,\n          6.4862e-03, -1.4120e-02,  1.7721e-02, -1.4591e-02, -2.0929e-02,\n         -9.6128e-03, -4.2325e-03, -1.2399e-02,  4.0044e-03,  1.9648e-02,\n         -2.3836e-02,  2.9528e-03,  2.1263e-03, -1.0291e-02,  6.1070e-03],\n        [ 1.3660e-02,  1.2023e-02,  1.4274e-02, -1.5708e-02, -5.5309e-03,\n          1.6881e-03,  1.4524e-02,  5.2179e-03, -8.8621e-03, -9.8444e-03,\n         -5.7476e-03, -3.5837e-03,  6.7017e-03,  1.7840e-02, -9.8739e-03,\n         -6.2555e-03, -1.2688e-02, -1.3724e-02,  2.2323e-02, -3.4205e-04,\n          1.1016e-02, -3.5792e-03, -8.4602e-03,  6.5782e-03,  2.3950e-02,\n         -1.3159e-02, -4.7241e-03,  7.4166e-03,  1.4312e-02, -2.1230e-03],\n        [ 5.5496e-03,  8.7048e-04,  9.7143e-04,  5.5974e-03, -2.3591e-05,\n          8.1295e-03,  1.1833e-02, -4.5214e-03, -1.1665e-02,  8.6588e-03,\n          3.8855e-03, -6.2617e-03,  8.0384e-03, -1.0396e-02, -1.2053e-02,\n         -6.1403e-03,  1.8917e-02, -6.9527e-03, -5.4443e-03,  7.3035e-03,\n         -5.0620e-03,  8.4469e-03, -7.6108e-03, -3.8455e-03,  5.2000e-03,\n          5.8073e-04, -1.1567e-02, -5.4047e-03,  1.6713e-03, -8.0399e-03],\n        [-3.3293e-03,  3.9605e-03, -1.1601e-02, -2.6038e-03, -1.1692e-02,\n          1.1394e-02, -9.8606e-03, -1.4142e-02,  9.6771e-03,  1.2687e-02,\n          1.4138e-03, -1.3122e-02, -7.2983e-03, -1.0236e-02,  5.0109e-04,\n         -2.2393e-03,  2.7749e-03,  6.8510e-03,  5.7523e-03, -2.1917e-03,\n          6.6004e-03,  4.1726e-03,  1.6834e-02,  1.0669e-02, -3.7315e-03,\n         -8.6138e-03, -6.2946e-03, -9.9173e-04, -6.2338e-03, -1.0307e-02],\n        [ 5.2632e-03,  5.3373e-03, -1.5287e-03,  1.4665e-02,  2.7480e-03,\n         -1.9429e-03,  1.6641e-03,  3.7625e-03, -7.3227e-03, -1.0563e-03,\n         -3.9787e-03, -1.6089e-02,  1.0245e-02,  1.2784e-02,  1.7988e-02,\n          1.0817e-03,  7.6124e-03,  1.0776e-02, -3.2118e-03,  1.9478e-02,\n         -1.6235e-02,  2.1501e-03,  4.4740e-03, -5.0982e-03,  9.5525e-03,\n          8.0629e-03, -2.0781e-03, -1.1124e-02,  6.7687e-03, -1.1606e-04],\n        [-2.7141e-03,  8.7898e-03,  1.3361e-03,  2.1362e-02,  2.8660e-03,\n         -5.2622e-04, -1.5546e-02, -1.8686e-02, -8.4874e-03,  6.1482e-03,\n         -2.1157e-03,  2.1162e-02, -5.9596e-03,  2.2795e-03,  2.7729e-03,\n          4.3917e-03,  2.0229e-02,  1.4855e-02, -2.6214e-03, -2.4920e-02,\n         -6.4848e-03, -1.6814e-03, -1.7142e-02,  2.3125e-02, -9.3672e-03,\n          4.4122e-03, -2.3972e-03,  1.5565e-02,  5.0688e-03,  9.8665e-03],\n        [-1.1150e-02,  9.9511e-03, -5.9282e-03, -8.0559e-03, -1.0204e-03,\n          1.0025e-02,  7.5559e-03, -2.0546e-02,  1.6310e-03,  2.1827e-02,\n          1.0804e-02, -1.5636e-02, -3.2779e-03,  4.6190e-03,  1.1566e-03,\n         -2.9698e-03,  4.8967e-03,  7.7357e-03,  1.4349e-02, -1.9982e-03,\n          1.1614e-02,  6.2583e-03,  9.5189e-04, -2.1682e-03,  1.9656e-02,\n          5.1398e-03,  1.5367e-02, -7.0628e-03,  5.2249e-03, -1.2779e-02]])\n2.linear.weighttensor([[-9.3091e-03, -1.2619e-02,  1.7688e-02, -2.2170e-03,  1.4489e-03,\n          4.3294e-03, -1.1287e-02,  5.2824e-03, -6.2739e-03,  6.6699e-03,\n         -1.0098e-02,  1.4475e-02, -1.4330e-02, -3.6510e-03, -1.7171e-03,\n         -1.8049e-03, -4.1736e-03, -4.8391e-03,  7.7752e-03,  5.2586e-03],\n        [-9.0335e-03,  1.1235e-02, -1.3549e-02, -7.4261e-03, -6.1727e-03,\n         -2.4140e-03,  1.2253e-02, -2.5733e-03,  1.0141e-02, -1.6034e-02,\n         -9.0091e-03,  9.0712e-03, -6.0820e-03,  6.3926e-03,  8.2096e-03,\n         -2.2053e-02,  2.0514e-03,  6.7102e-03,  7.4148e-03,  1.2139e-02],\n        [-8.9624e-03,  8.9306e-03,  8.7590e-03, -3.3134e-03,  9.7330e-04,\n          1.3876e-02,  1.4476e-02, -6.7130e-03, -1.1780e-03,  4.6035e-03,\n          7.4368e-03, -5.7345e-03,  7.9627e-03,  3.7482e-03,  8.5412e-04,\n         -3.8640e-03,  1.6957e-04,  1.2457e-03, -1.1622e-02,  9.3359e-03],\n        [-1.7593e-02,  5.9211e-03,  7.7413e-03,  6.4380e-03,  8.0821e-03,\n         -4.8565e-03,  1.5844e-02,  1.1448e-02,  1.2921e-02,  1.3551e-02,\n          1.1770e-02, -9.6656e-03, -1.4743e-02, -1.5506e-03,  3.6256e-03,\n          7.8754e-03,  2.3375e-03,  1.4382e-03, -1.4841e-02,  8.1830e-04],\n        [-4.3054e-03,  8.9312e-03, -5.6219e-03, -7.8774e-03, -1.2079e-02,\n          1.9396e-02, -1.1059e-02,  1.1763e-02,  6.4136e-04,  4.6686e-03,\n          1.7265e-03, -5.4181e-03, -5.4554e-03,  7.0432e-03, -1.4839e-02,\n          6.1487e-03,  3.3533e-03, -6.8792e-05,  2.4205e-02,  1.7291e-02],\n        [-2.1115e-02, -1.1523e-02,  3.8706e-03,  1.3529e-03, -3.6744e-02,\n          4.7976e-03, -1.2864e-03,  1.5287e-02, -2.4601e-02, -9.9966e-03,\n         -8.5414e-03, -1.5620e-02,  1.7209e-02, -1.5794e-02, -2.1189e-02,\n         -8.5709e-04,  9.0813e-03, -1.4233e-02,  3.9600e-03,  6.1287e-03],\n        [-5.4590e-03,  1.0257e-02,  1.3215e-02, -4.8236e-03,  6.1571e-03,\n          1.4622e-03,  8.0316e-03,  2.1034e-03, -9.9236e-03, -9.6954e-03,\n          2.5919e-02, -1.1296e-02,  1.3127e-02,  2.3757e-03, -1.3511e-02,\n          3.7322e-03,  3.6803e-03,  4.0958e-04, -2.5720e-03,  5.2057e-03],\n        [ 6.2571e-04,  1.1178e-03, -1.7439e-02, -1.8469e-03,  1.3318e-02,\n         -9.3696e-03,  1.9281e-03, -3.9099e-03,  5.3598e-03, -3.8684e-03,\n          3.1857e-03,  1.0787e-02,  8.6631e-03, -1.0392e-02, -2.0631e-02,\n         -9.7854e-03, -1.2424e-02, -1.9982e-02,  1.0775e-02, -1.3784e-03],\n        [-1.8829e-02, -1.5088e-02, -6.8635e-03, -9.6378e-04, -4.6430e-03,\n          3.9646e-03, -8.3993e-03, -7.2074e-03,  1.3270e-02,  6.6709e-03,\n          2.6894e-03,  1.2195e-02, -1.6004e-02,  2.3378e-02,  9.8157e-03,\n          2.4159e-03,  9.6602e-03, -2.9804e-03, -1.4090e-03, -1.5315e-02],\n        [-1.0243e-02, -8.5150e-03, -1.6737e-02,  6.6467e-03, -4.8033e-03,\n          1.7858e-02, -9.8422e-03,  7.4112e-03, -9.3375e-03,  7.8257e-03,\n          1.0774e-02, -1.0465e-02, -4.4401e-03, -1.3916e-02,  8.7693e-03,\n          8.6185e-03, -1.1482e-02,  6.2975e-03,  2.4839e-04, -1.0796e-02],\n        [-2.2737e-03, -1.0891e-02,  8.5816e-03,  7.3370e-04, -2.1881e-02,\n          1.5661e-02,  3.2561e-03,  2.4255e-04, -1.1917e-02, -1.4963e-03,\n         -8.5354e-03,  2.3027e-03,  4.9417e-03,  6.6041e-03,  2.6012e-03,\n         -1.8020e-02,  6.0770e-03, -1.2694e-02,  1.8117e-02,  1.3967e-04],\n        [-3.7284e-03, -8.6628e-03,  1.0966e-02, -1.0040e-02,  3.9479e-03,\n         -1.0920e-02, -9.5461e-03, -9.6466e-04,  3.4316e-03,  1.7798e-02,\n         -5.2909e-03, -1.8953e-02, -2.7086e-02,  8.2350e-03, -3.6641e-03,\n          6.3067e-03,  1.2912e-02, -7.7205e-03,  1.1447e-02,  1.3138e-02],\n        [ 6.7947e-03,  1.8917e-03, -8.9776e-03, -9.4497e-03,  4.6993e-03,\n         -1.3277e-02, -6.7619e-03,  6.1313e-03, -1.1940e-02, -1.1576e-02,\n         -9.4921e-03, -4.7592e-03, -8.7366e-03, -1.3188e-03, -8.1955e-03,\n          8.4064e-03,  9.6219e-03, -3.0999e-03,  7.9558e-03,  5.7783e-03],\n        [-1.7316e-03,  1.3007e-02,  9.7634e-03, -4.9637e-03,  1.7185e-04,\n         -9.6393e-03,  1.6932e-03, -4.4323e-03,  6.3563e-03, -4.1034e-03,\n          1.5057e-03,  9.7530e-03, -9.5403e-03,  1.3540e-02,  1.1281e-02,\n          8.4492e-03,  1.3962e-02, -8.1984e-03, -2.8244e-03,  6.2209e-03],\n        [ 3.8196e-03, -6.9281e-04,  3.9473e-03,  2.9684e-03, -4.7141e-03,\n         -5.9569e-03, -9.8936e-03, -1.5115e-04, -2.3006e-02,  9.7396e-03,\n          5.0272e-04, -1.5208e-02,  5.9712e-03,  2.5914e-02, -3.6040e-03,\n         -7.0914e-03,  4.9005e-04, -1.2981e-02,  1.0792e-02,  1.8668e-02],\n        [-1.2279e-02,  2.2029e-03, -9.0931e-03, -2.6080e-03,  6.6961e-03,\n         -1.4611e-02,  3.0719e-02,  5.1979e-03,  1.3267e-02,  1.1507e-02,\n          9.5269e-03, -1.8561e-02, -1.2505e-02,  1.3722e-02,  8.6717e-03,\n          1.6062e-02, -1.9447e-03, -2.7202e-02, -1.6187e-02,  9.7472e-03],\n        [-1.3429e-02,  1.2115e-02, -3.6527e-03, -6.8123e-03,  1.9255e-03,\n         -1.4687e-03,  1.2725e-03,  4.6134e-03, -1.2398e-02,  1.6287e-03,\n         -1.1646e-02,  3.0192e-03,  1.1897e-02, -8.6480e-03, -6.9941e-03,\n          1.0988e-02,  1.7476e-02, -3.6279e-03,  3.9971e-03, -2.0421e-03],\n        [-4.5693e-03,  7.5648e-03, -1.7466e-02, -4.8380e-03, -3.0409e-02,\n          3.7131e-03, -4.6505e-03,  1.7149e-03,  3.7374e-03, -2.5013e-02,\n         -1.0919e-02,  3.1132e-03,  6.3445e-03, -2.1203e-02, -1.5276e-02,\n          7.2470e-04, -1.0106e-02, -2.5524e-03, -3.6756e-04,  1.3486e-02],\n        [ 1.7625e-03,  1.3613e-02,  9.3838e-03, -1.0607e-02,  1.1133e-03,\n         -4.5643e-03,  1.6004e-04, -4.7657e-03, -1.8886e-03, -1.4965e-02,\n         -7.1867e-03,  4.2407e-03, -3.8393e-03,  1.9056e-03, -1.0160e-02,\n         -1.3916e-02, -3.4785e-03, -5.3357e-03, -1.9046e-02,  3.2077e-03],\n        [-3.6408e-04, -1.4191e-02,  1.6300e-03,  5.8447e-03, -6.8427e-03,\n         -1.8483e-02,  1.9487e-03, -1.2881e-02, -1.1746e-03, -1.4508e-02,\n         -2.9369e-03, -3.1037e-03,  3.2146e-03, -2.0236e-02,  7.4765e-03,\n          2.1671e-03,  4.7461e-03, -5.4471e-03,  1.4448e-02,  3.6452e-04]])\n"}],"source":"from torch.nn import init\nfor name, param in nestModule.named_parameters():\n    if 'weight' in name: \n        init.normal_(param, mean=0, std=0.01)\n        print(name, param.data)"},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0.net.0.bias\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0.], requires_grad=True)\n1.bias\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       requires_grad=True)\n2.linear.bias\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       requires_grad=True)\n"}],"source":"for name, param in nestModule.named_parameters():\n    if 'bias' in name:\n        init.constant_(param, val=0)\n        print(name, '\\n', param)"},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0.net.0.weight\nParameter containing:\ntensor([[ 0.0000, -8.2964, -0.0000,  ..., -7.9558,  0.0000, -0.0000],\n        [-0.0000,  0.0000,  7.5248,  ...,  8.4141,  7.5898,  0.0000],\n        [ 6.5668, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -6.3137],\n        ...,\n        [-6.3530,  6.5531,  6.2616,  ...,  0.0000,  6.6434,  8.4557],\n        [ 0.0000, -8.7917,  0.0000,  ..., -6.8890,  6.4532, -0.0000],\n        [ 0.0000,  0.0000,  6.5248,  ...,  6.5456, -0.0000, -8.6588]],\n       requires_grad=True)\n1.weight\nParameter containing:\ntensor([[-0.0000, -9.7025,  0.0000,  8.6028,  0.0000,  7.9704, -8.1251,  8.3370,\n         -8.1020, -0.0000, -0.0000, -0.0000, -9.5632, -9.2052, -6.7319,  6.6002,\n          9.0449,  0.0000, -9.3713, -5.1876, -0.0000,  6.5189,  0.0000,  5.6039,\n          0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n        [ 6.6082, -7.1986,  0.0000,  7.2345, -0.0000,  6.5871,  0.0000,  0.0000,\n         -8.5550, -0.0000,  6.1927,  0.0000,  0.0000,  5.5509, -6.4474, -0.0000,\n          7.9013,  0.0000,  0.0000,  9.4159,  0.0000,  0.0000,  6.0673, -9.9384,\n         -0.0000,  0.0000,  9.2258,  5.3230,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -5.8990, -9.4549, -8.2240,  6.1392, -0.0000, -7.9251,\n          0.0000, -5.4995, -0.0000, -0.0000,  0.0000, -8.4913, -7.7132,  0.0000,\n         -9.6903,  0.0000, -0.0000, -9.5676, -0.0000, -0.0000,  5.5569, -0.0000,\n          0.0000,  0.0000, -0.0000,  9.9667, -0.0000,  0.0000],\n        [-0.0000,  8.1365, -6.2856,  0.0000,  0.0000, -0.0000, -0.0000, -9.0592,\n         -0.0000, -0.0000, -9.9493, -8.1176, -7.5140, -0.0000, -0.0000, -0.0000,\n         -8.3985, -0.0000,  0.0000,  6.0871,  0.0000,  8.1661, -0.0000,  6.4770,\n         -6.0528, -6.2260,  0.0000, -5.0301, -0.0000,  8.1806],\n        [ 9.0567,  8.7519, -0.0000, -8.3360, -9.0626,  0.0000,  8.4750,  0.0000,\n          9.5170, -0.0000, -0.0000,  6.8526,  0.0000,  6.9281,  0.0000, -0.0000,\n         -0.0000, -0.0000, -7.3499,  7.4224,  9.0040, -0.0000, -0.0000, -0.0000,\n          0.0000,  7.1219, -8.2150,  0.0000, -8.0135,  7.0224],\n        [ 0.0000, -7.8888, -5.6072, -0.0000,  7.2139,  0.0000, -0.0000, -0.0000,\n         -0.0000, -0.0000,  0.0000, -0.0000, -7.3833,  8.1665, -0.0000,  0.0000,\n         -0.0000,  5.3887,  0.0000, -0.0000, -6.6627, -6.8369,  0.0000,  0.0000,\n          0.0000,  8.8554, -0.0000, -5.0750,  0.0000, -8.5021],\n        [-0.0000, -0.0000,  9.9147,  0.0000,  7.4696,  8.5354,  0.0000, -7.5678,\n          0.0000,  5.4629,  7.4663,  0.0000,  8.4151,  0.0000,  6.3805, -7.8013,\n          0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -5.4891,  0.0000,\n         -8.6083,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n        [-5.6921, -9.0897,  8.9337, -0.0000, -8.4882, -5.2725,  6.5765, -8.9992,\n          5.3441, -0.0000,  0.0000,  6.3104, -0.0000,  6.1696, -0.0000, -7.1930,\n         -8.5118, -0.0000, -7.4888,  7.1897,  0.0000, -7.6634,  8.0054, -9.6325,\n          0.0000, -0.0000, -7.1979, -0.0000, -0.0000, -0.0000],\n        [-7.0947, -5.3934,  6.4597, -9.8119, -7.7064, -0.0000,  0.0000, -0.0000,\n         -0.0000,  6.8249, -0.0000, -0.0000,  0.0000,  0.0000,  5.2564, -8.0375,\n          0.0000,  7.4818, -7.1848,  6.7473, -5.1996, -0.0000,  7.3162, -0.0000,\n          8.5477, -9.2965,  0.0000, -0.0000, -0.0000, -8.3867],\n        [-5.6313, -9.2929,  5.0183, -8.5488, -7.0284, -8.5184,  0.0000,  0.0000,\n         -8.2677,  0.0000, -0.0000,  0.0000, -5.2412, -0.0000, -0.0000, -0.0000,\n          0.0000,  7.2124,  0.0000,  0.0000, -7.8611, -0.0000, -8.6104, -6.3843,\n          0.0000, -0.0000, -5.5328,  9.3284,  0.0000,  0.0000],\n        [ 7.7641, -5.7257, -0.0000, -0.0000,  5.0284,  0.0000,  0.0000, -0.0000,\n         -7.6429, -9.8281, -6.0899, -0.0000, -5.6081,  9.3157, -0.0000,  8.2581,\n         -0.0000,  0.0000, -0.0000, -8.6328,  5.6596,  0.0000,  6.7378, -8.5750,\n         -0.0000, -0.0000,  0.0000, -6.4694,  8.4708,  9.0397],\n        [ 6.3378,  0.0000,  0.0000, -7.3184, -0.0000, -9.8761,  7.7748,  0.0000,\n          9.0120, -0.0000, -9.8645,  0.0000,  8.7623, -8.5322,  6.2264, -8.2926,\n         -5.5572,  7.1876, -6.8400,  6.0448,  5.1745,  0.0000,  0.0000, -0.0000,\n          0.0000, -0.0000,  0.0000, -5.6617, -9.8841, -0.0000],\n        [ 0.0000, -0.0000,  0.0000, -8.3779, -5.0104,  0.0000,  7.6649,  0.0000,\n         -6.1556,  7.0831, -8.1414, -5.6897, -6.1280, -0.0000, -6.1356, -5.9487,\n         -0.0000,  9.6934, -0.0000, -0.0000, -0.0000,  0.0000,  5.8113,  0.0000,\n          5.3257, -7.5300, -6.1559, -0.0000, -7.6823, -0.0000],\n        [-7.5208, -8.7232,  0.0000, -0.0000,  8.9031,  5.6792, -5.5068, -6.8427,\n          6.8928,  0.0000, -0.0000, -0.0000, -9.5489, -7.8211,  7.7153,  0.0000,\n         -0.0000,  9.9686, -7.2041,  0.0000,  0.0000, -0.0000, -5.0135,  0.0000,\n         -0.0000, -0.0000, -0.0000,  9.2900,  0.0000, -9.7860],\n        [ 6.5767, -9.2788,  0.0000,  7.8300, -9.2361,  5.0100, -8.4176,  5.1097,\n         -7.0590, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n         -0.0000,  0.0000,  0.0000,  0.0000, -6.2926, -0.0000, -7.8852, -9.4314,\n         -0.0000, -9.8061, -0.0000, -0.0000,  0.0000, -7.6346],\n        [ 9.9869,  0.0000, -0.0000,  9.1337, -0.0000,  0.0000, -0.0000, -5.4514,\n         -0.0000, -5.6714, -8.0468,  5.4244, -5.6931,  9.7461, -0.0000,  8.6233,\n          0.0000, -6.3470, -6.3294, -7.6270, -9.5515, -0.0000,  0.0000,  0.0000,\n         -8.7952,  8.7803,  0.0000, -9.8543, -6.4678, -0.0000],\n        [ 9.0327, -0.0000, -6.8763, -0.0000,  7.0329,  8.1844,  0.0000,  7.3852,\n          9.0669,  7.5730,  0.0000,  8.4964,  0.0000,  0.0000,  8.6721, -9.3313,\n          8.9923, -6.9192, -9.6034,  8.1435,  9.9029,  0.0000, -0.0000,  8.8717,\n         -5.2111, -5.7237, -7.6826,  0.0000, -8.0590, -0.0000],\n        [-0.0000,  0.0000, -0.0000,  0.0000,  5.4624,  0.0000,  0.0000, -7.8715,\n         -6.8730,  7.1110, -0.0000,  0.0000, -0.0000, -7.4758, -5.8320, -6.0789,\n         -0.0000,  7.6473,  7.2998, -0.0000, -9.8332,  8.0702,  9.4150,  9.8582,\n         -0.0000, -0.0000,  0.0000,  5.9911, -5.0856,  0.0000],\n        [-7.9445, -0.0000, -9.6799,  0.0000, -8.8161,  8.0531, -9.9471,  6.9077,\n          6.5184, -0.0000,  7.7917,  5.6501,  0.0000, -0.0000, -0.0000,  7.8544,\n         -7.6601, -6.8430,  5.4061, -9.5147,  0.0000,  0.0000, -6.8868, -6.3772,\n          9.3415, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n        [-8.4801, -0.0000, -8.2168,  8.9113,  9.9128,  0.0000, -7.4759, -6.0860,\n         -0.0000, -7.6656,  7.9863,  7.1940,  5.9742,  5.3989, -0.0000,  9.8380,\n         -0.0000,  9.7146,  0.0000,  5.8765,  0.0000, -0.0000,  0.0000, -5.6305,\n         -6.6529,  0.0000,  0.0000,  8.0967,  0.0000,  6.7495]],\n       requires_grad=True)\n2.linear.weight\nParameter containing:\ntensor([[ 0.0000,  0.0000,  0.0000,  8.2423, -9.8716,  9.3679, -6.6284, -0.0000,\n         -0.0000,  6.7121,  7.1197, -8.0297,  7.3411,  7.4049,  0.0000, -5.6161,\n         -8.0565,  5.5928,  8.3342,  7.3117],\n        [-0.0000, -7.2365, -0.0000, -0.0000,  9.6464,  0.0000,  0.0000,  7.0142,\n          0.0000, -8.3868,  8.9567, -7.5221, -7.8635, -8.7968, -6.8224, -9.8005,\n          7.8311,  0.0000,  0.0000,  9.6514],\n        [-0.0000, -0.0000, -0.0000,  0.0000,  9.6225, -8.1281, -9.1826, -0.0000,\n          5.4555,  9.9585,  0.0000, -7.0046,  7.5207, -6.1524,  6.7885,  9.3993,\n         -0.0000,  0.0000, -5.9227, -0.0000],\n        [ 7.6779, -0.0000,  0.0000, -0.0000,  0.0000, -9.8463,  0.0000,  7.1560,\n         -9.1257,  8.9360,  5.6877, -6.9201, -6.4875,  7.3831, -0.0000, -8.4573,\n         -7.7948,  8.6040, -0.0000,  8.4008],\n        [-0.0000,  8.7056,  0.0000,  0.0000, -0.0000,  0.0000,  7.1302, -0.0000,\n          0.0000,  0.0000, -9.4569,  7.2214,  6.0985,  0.0000,  7.4949,  0.0000,\n          8.5469, -5.0577, -0.0000, -6.6703],\n        [-0.0000, -6.9054, -0.0000,  0.0000, -6.7192, -0.0000,  5.1475,  0.0000,\n         -0.0000, -6.5863, -7.6119, -6.7847,  8.9529, -7.7644,  7.9790, -9.4712,\n         -0.0000,  8.4711, -8.3382, -8.1080],\n        [ 0.0000,  7.4863, -7.5112,  0.0000,  8.0635,  0.0000, -0.0000,  7.8029,\n          0.0000, -6.9021, -8.1986, -0.0000,  9.9181,  5.7062,  6.0956,  0.0000,\n          5.9527, -0.0000, -0.0000,  7.4139],\n        [-8.2632, -0.0000,  9.2272, -0.0000, -0.0000,  6.4572, -0.0000, -0.0000,\n          8.0843,  8.2595,  8.5039,  0.0000, -9.3878,  0.0000,  0.0000,  5.9224,\n          0.0000, -9.6124, -9.8388, -6.2573],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -5.5096,\n          9.4100, -0.0000,  5.7373, -0.0000,  8.6679,  0.0000, -0.0000, -0.0000,\n         -0.0000,  0.0000,  7.6908,  0.0000],\n        [-9.8534, -9.7768, -0.0000,  0.0000, -8.9298, -8.8826, -7.2496, -0.0000,\n          0.0000,  5.5129,  0.0000,  5.9997,  0.0000,  0.0000,  7.2200, -5.2979,\n          0.0000, -9.4914,  0.0000,  7.4517],\n        [-0.0000, -0.0000, -0.0000, -6.3335, -0.0000,  5.4494,  0.0000,  7.0179,\n         -0.0000, -0.0000,  0.0000, -5.8420, -0.0000,  9.2158, -0.0000, -5.7770,\n         -0.0000,  9.1514,  5.9730,  0.0000],\n        [-0.0000, -6.3567,  7.3129,  8.6376,  7.3439, -0.0000, -6.2437,  7.5811,\n         -0.0000,  6.6222, -0.0000,  9.8969,  5.4345,  0.0000,  6.8415, -0.0000,\n         -0.0000, -7.5731, -0.0000, -8.3928],\n        [-0.0000, -6.2205,  0.0000,  0.0000, -5.4583, -6.5026,  0.0000, -5.8650,\n         -6.1758,  0.0000,  0.0000,  7.9953,  0.0000, -0.0000, -0.0000, -0.0000,\n          0.0000, -6.7031,  8.5022, -8.6441],\n        [-9.6429, -0.0000,  9.8278, -0.0000,  0.0000, -0.0000,  9.5325,  0.0000,\n         -5.4321,  7.4031,  9.4796,  9.3862,  0.0000, -7.3598,  0.0000, -0.0000,\n         -7.6008, -0.0000,  0.0000, -0.0000],\n        [ 5.8745, -0.0000, -5.7564,  6.7899,  7.2226,  0.0000, -6.6809,  0.0000,\n         -0.0000, -9.8319,  8.9682,  0.0000, -0.0000,  8.0644,  6.7071,  7.6124,\n         -0.0000,  0.0000, -0.0000,  9.9577],\n        [ 6.6634, -7.1729, -0.0000, -6.3825,  7.1489, -8.6528, -0.0000,  0.0000,\n          0.0000,  6.5035,  0.0000,  8.0352,  0.0000, -8.6968,  0.0000, -7.4810,\n         -7.1267, -9.5595, -0.0000,  5.1375],\n        [-5.1988, -0.0000,  7.2687, -7.7379,  9.5655,  0.0000, -8.2244,  6.9235,\n          7.7095, -7.8543, -0.0000, -0.0000, -9.7653, -7.5686, -9.8575,  8.3809,\n         -9.5756, -0.0000, -7.6150, -8.9039],\n        [ 5.1049, -0.0000, -9.5730,  7.2560, -0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  5.1800,  0.0000, -0.0000, -6.2757, -0.0000,  0.0000,  6.3790,\n         -0.0000, -5.3142,  0.0000,  8.8713],\n        [ 0.0000, -0.0000, -8.8868,  6.2024,  9.2845,  8.3341, -0.0000, -7.3081,\n         -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -5.4768,  0.0000,  9.6492,\n          0.0000, -6.2921,  9.7149,  6.1572],\n        [ 8.0928, -9.3973, -6.4187,  0.0000,  0.0000, -7.3957,  0.0000,  9.9795,\n          6.2350, -0.0000,  5.1493,  0.0000, -6.0125, -0.0000, -0.0000, -0.0000,\n          0.0000, -0.0000,  8.2271,  0.0000]], requires_grad=True)\n"}],"source":"# 自定义初始化方法\ndef init_weight_(tensor):\n    with torch.no_grad():\n         tensor.uniform_(-10, 10)   # inplace 操作\n         tensor *= (tensor.abs() >= 5).float()\n\nfor name, param in nestModule.named_parameters():\n    if 'weight' in name:\n        init_weight_(param)\n        print(name, '\\n',  param)"},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0.net.0.bias\ntensor([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.])\n1.bias\ntensor([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n        4., 4.])\n2.linear.bias\ntensor([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n        4., 4.])\n"}],"source":"# 改变参数值 但是不影响梯度: 使用data属性，不会被记录在计算图中\nfor name, param in nestModule.named_parameters():\n    if 'bias' in name:\n        param.data += 1\n        print(name, '\\n', param.data)"},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=False)\n  (1): Linear(in_features=2, out_features=2, bias=False)\n  (2): Linear(in_features=2, out_features=2, bias=False)\n)\n0.weight\ntensor([[3., 3.],\n        [3., 3.]])\nTrue\n"}],"source":"##### 共享模型参数， 因为模型参数里包含了梯度，所以在反向传播计算时，这些共享的梯度是累加的\nlinear = nn.Linear(2,2, bias=False)\nnet = nn.Sequential(linear, linear, linear)\nprint(net)\n\nfor name, param in net.named_parameters():\n    init.constant_(param, val=3)\n    print(name, '\\n', param.data)\n\nprint(id(net[0]) == id(net[1]))        # 两个线性层其实是一个对象\n"},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor(864., grad_fn=<SumBackward0>)\ntensor([[216., 216.],\n        [216., 216.]])\n"}],"source":"x = torch.ones(2,2)\ny = net(x).sum()\nnet.zero_grad()\ny.backward()\nprint(y, '\\n', net[0].weight.grad)"},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":"#################################################### 自定义层： nn.Module\n\n### 不含模型参数的自定义层\nclass CenteredLayer(nn.Module):\n    def __init__(self, **kwards):\n        super(CenteredLayer, self).__init__(**kwards)\n    def forward(self, x):\n        return x - x.mean()\n"},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[ 0.2500,  0.4736,  0.3497,  0.0024,  0.3314, -0.2915,  0.3861, -0.4774,\n         -0.2092, -0.3287],\n        [-0.4933, -0.3188, -0.0129,  0.2012,  0.0104,  0.3017, -0.0969,  0.2819,\n          0.1212, -0.4554],\n        [-0.4830, -0.2102,  0.1164,  0.0963,  0.4228,  0.3864,  0.4056,  0.4598,\n         -0.5140, -0.1571],\n        [ 0.0879, -0.0751,  0.1490,  0.0675,  0.2524,  0.0194, -0.5143,  0.2055,\n         -0.1395,  0.1076],\n        [ 0.3162, -0.1657,  0.1506,  0.4765, -0.3470, -0.0255, -0.4793, -0.4856,\n          0.0706, -0.0695],\n        [ 0.3854,  0.4766,  0.1097, -0.0528, -0.1431, -0.1592,  0.3191, -0.0310,\n          0.3549, -0.2005],\n        [-0.2625,  0.0125,  0.2340,  0.2316, -0.2667, -0.3291, -0.4060, -0.0379,\n          0.0322, -0.0586],\n        [-0.2046, -0.3730,  0.4627, -0.4936,  0.1201,  0.3746, -0.0205, -0.5010,\n         -0.0655, -0.2492],\n        [-0.0530,  0.1380,  0.1048,  0.4034, -0.3485,  0.4689,  0.2296,  0.0114,\n         -0.0359, -0.4422],\n        [ 0.1902,  0.4739,  0.1559, -0.3845, -0.2376,  0.2309,  0.1244,  0.3435,\n         -0.4033, -0.3786]])\n"}],"source":"layer = CenteredLayer()\ninput = torch.rand(10,10,dtype=torch.float32)\noutput = layer(input)\nprint(output)"},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"-3.725290298461914e-09\n"}],"source":"net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())\ninput = torch.rand(4, 8)\noutput = net(input)\nprint(output.mean().item())"},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"4\n3\nParameter containing:\ntensor([[-0.8814, -0.3411, -0.0116,  0.5605],\n        [-2.9033,  0.7493,  1.9043, -0.0604],\n        [ 0.7425, -0.3993, -1.3453, -1.7525],\n        [ 0.2149,  0.4505,  0.5775,  0.5321]], requires_grad=True)\n"}],"source":"### 包含模型参数的自定义层\n# 如果一个Tensor是Parameter，会被自动添加到模型的参数列表中。 定义参数：Parameter、ParameterList、parameterDict\n\nclass MyDense(nn.Module):\n    def __init__(self):\n        super(MyDense, self).__init__()\n        self.lstparams = nn.ParameterList([nn.Parameter(torch.randn(4,4)) for i in range(3)])\n        self.lstparams.append(nn.Parameter(torch.randn(4,1)))\n\n        self.dictparams = nn.ParameterDict({\n            'linear1' : nn.Parameter(torch.randn(4,4)),\n            'linear2' : nn.Parameter(torch.randn(4,1))\n        })\n        self.dictparams.update({'linear3': nn.Parameter(torch.randn(4, 2))})\n    def forward(self, x):\n        for i in range(len(self.lstparams)):\n            x = torch.mm(x, self.lstparams[i])\n        for name, param in self.dictparams.items():\n            x = torch.mm(x, param)\n        return x\nnet = MyDense()\nprint(len(net.lstparams), '\\n', len(net.dictparams.keys()))\nprint(net.dictparams['linear1'])"},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Sequential(\n  (0): MyDense(\n    (lstparams): ParameterList(\n        (0): Parameter containing: [torch.FloatTensor of size 4x4]\n        (1): Parameter containing: [torch.FloatTensor of size 4x4]\n        (2): Parameter containing: [torch.FloatTensor of size 4x4]\n        (3): Parameter containing: [torch.FloatTensor of size 4x1]\n    )\n    (dictparams): ParameterDict(\n        (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n        (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n        (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n    )\n  )\n  (1): MyDense(\n    (lstparams): ParameterList(\n        (0): Parameter containing: [torch.FloatTensor of size 4x4]\n        (1): Parameter containing: [torch.FloatTensor of size 4x4]\n        (2): Parameter containing: [torch.FloatTensor of size 4x4]\n        (3): Parameter containing: [torch.FloatTensor of size 4x1]\n    )\n    (dictparams): ParameterDict(\n        (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n        (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n        (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n    )\n  )\n)\n"}],"source":"net = nn.Sequential(MyDense(), MyDense())\nprint(net)"},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0.lstparams.0\n0.lstparams.1\n0.lstparams.2\n0.lstparams.3\n0.dictparams.linear1\n0.dictparams.linear2\n0.dictparams.linear3\n1.lstparams.0\n1.lstparams.1\n1.lstparams.2\n1.lstparams.3\n1.dictparams.linear1\n1.dictparams.linear2\n1.dictparams.linear3\n"}],"source":"############################# 模型的读取和存储\n# 只有具有可学习参数的层才有state_dict中的条目\nfor key, value in net.state_dict().items():\n    print(key)"},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"1\n0\n(6, 1)GeForce GTX 1070 Ti\n"}],"source":"print(torch.cuda.device_count())\nprint(torch.cuda.current_device())\nprint(torch.cuda.get_device_capability(), torch.cuda.get_device_name())"},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"cuda:0\n"},{"data":{"text/plain":"Sequential(\n  (0): MyDense(\n    (lstparams): ParameterList(\n        (0): Parameter containing: [torch.cuda.HalfTensor of size 4x4 (GPU 0)]\n        (1): Parameter containing: [torch.cuda.HalfTensor of size 4x4 (GPU 0)]\n        (2): Parameter containing: [torch.cuda.HalfTensor of size 4x4 (GPU 0)]\n        (3): Parameter containing: [torch.cuda.HalfTensor of size 4x1 (GPU 0)]\n    )\n    (dictparams): ParameterDict(\n        (linear1): Parameter containing: [torch.cuda.HalfTensor of size 4x4 (GPU 0)]\n        (linear2): Parameter containing: [torch.cuda.HalfTensor of size 4x1 (GPU 0)]\n        (linear3): Parameter containing: [torch.cuda.HalfTensor of size 4x2 (GPU 0)]\n    )\n  )\n  (1): MyDense(\n    (lstparams): ParameterList(\n        (0): Parameter containing: [torch.cuda.HalfTensor of size 4x4 (GPU 0)]\n        (1): Parameter containing: [torch.cuda.HalfTensor of size 4x4 (GPU 0)]\n        (2): Parameter containing: [torch.cuda.HalfTensor of size 4x4 (GPU 0)]\n        (3): Parameter containing: [torch.cuda.HalfTensor of size 4x1 (GPU 0)]\n    )\n    (dictparams): ParameterDict(\n        (linear1): Parameter containing: [torch.cuda.HalfTensor of size 4x4 (GPU 0)]\n        (linear2): Parameter containing: [torch.cuda.HalfTensor of size 4x1 (GPU 0)]\n        (linear3): Parameter containing: [torch.cuda.HalfTensor of size 4x2 (GPU 0)]\n    )\n  )\n)"},"execution_count":96,"metadata":{},"output_type":"execute_result"}],"source":"device = torch.device(type='cuda', index=0)\nprint(device)\nnet.to(device, dtype=torch.float16)\n# 对在GPU上的数据进行运算，结果还是存放在GPU上。 存储在不同位置上的数据是不可以直接进行计算的"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}